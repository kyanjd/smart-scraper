{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 210173\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 168138\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 42035\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.8, seed=20)\n",
    "\n",
    "print(split_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 168138\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 42035\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "print(split_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'fr'],\n",
      "        num_rows: 168138\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['en', 'fr'],\n",
      "        num_rows: 42035\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def flatten_translation(examples):\n",
    "    return {\n",
    "        \"en\": [ex[\"en\"] for ex in examples[\"translation\"]],\n",
    "        \"fr\": [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    }\n",
    "\n",
    "equivalent_datasets = split_datasets.map(flatten_translation, batched=True, remove_columns=[\"id\", \"translation\"])\n",
    "\n",
    "print(equivalent_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1232, 13572, 7823, 9, 0], 'attention_mask': [1, 1, 1, 1, 1], 'labels': [22181, 10691, 412, 9, 1232, 21332, 0]}\n",
      "Web Shortcuts</s>\n",
      "[1232, 13572, 7823, 9, 0]\n",
      "Raccourcis WebComment</s>\n"
     ]
    }
   ],
   "source": [
    "en_sentence = equivalent_datasets[\"train\"][\"en\"][0]\n",
    "fr_sentence = equivalent_datasets[\"train\"][\"fr\"][0]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=fr_sentence)\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))\n",
    "print(tokenizer.encode(en_sentence))\n",
    "print(tokenizer.decode(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168138/168138 [00:44<00:00, 3768.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42035/42035 [00:11<00:00, 3803.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"en\"]\n",
    "    targets = examples[\"fr\"]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "# data_check = equivalent_datasets[\"train\"][0:4]\n",
    "# print(preprocess_function(data_check))\n",
    "tokenized_datasets_eq = equivalent_datasets.map(preprocess_function, batched=True, remove_columns=equivalent_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d98df5397684988bd12de9b215ab3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/168138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c29a029d7d4344aade0cb00b395109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42035 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 128\n",
    "def preprocess_function2(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "# data_check = split_datasets[\"train\"][0:10]\n",
    "# print(preprocess_function2(data_check))\n",
    "tokenized_datasets = split_datasets.map(preprocess_function2, batched=True, remove_columns=split_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168138/168138 [00:44<00:00, 3802.50 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42035/42035 [00:10<00:00, 3924.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "def preprocess_function3(examples):\n",
    "    inputs = examples[\"en\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"fr\"], max_length=max_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets_3 = equivalent_datasets.map(preprocess_function3, batched=True, remove_columns=equivalent_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1232, 13572, 7823, 9, 0], 'attention_mask': [1, 1, 1, 1, 1], 'labels': [22181, 10691, 412, 9, 1232, 21332, 0]}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets_eq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_datasets_eq[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_datasets_3[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_datasets_eq' is not defined"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0])\n",
    "print(tokenized_datasets_eq[\"train\"][0])\n",
    "print(tokenized_datasets_3[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c46b288197c4812a907437a899999b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c654733a0741ea92b5b298fc6b75d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n"
     ]
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1)])\n",
    "print(batch.keys())\n",
    "# print(batch[\"labels\"])\n",
    "# print(batch[\"input_ids\"])\n",
    "# print(batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds): \n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != 100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\kyanj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_login_key = os.environ.get(\"HF_LOGIN_KEY\")\n",
    "login(token=hf_login_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kyanj\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkj821\u001b[0m (\u001b[33mkj821-imperial-college-london\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb_api_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_api_key)\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"translation_test\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"checkpoint\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"Model_Files/translation-finetuning-test-en-fr\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=\"translation-finetuning-test-en-fr-v1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\kyanj\\Documents\\Repos\\smart-scraper\\src\\paper_to_equation\\T5\\wandb\\run-20250314_022410-aap6p7qc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kj821-imperial-college-london/translation_test/runs/aap6p7qc' target=\"_blank\">translation-finetuning-test-en-fr-v1</a></strong> to <a href='https://wandb.ai/kj821-imperial-college-london/translation_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kj821-imperial-college-london/translation_test' target=\"_blank\">https://wandb.ai/kj821-imperial-college-london/translation_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kj821-imperial-college-london/translation_test/runs/aap6p7qc' target=\"_blank\">https://wandb.ai/kj821-imperial-college-london/translation_test/runs/aap6p7qc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331947c3f5ea4aa987e282b8997771e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5892, 'grad_norm': 3.893394947052002, 'learning_rate': 9.968284173802727e-05, 'epoch': 0.01}\n",
      "{'loss': 1.482, 'grad_norm': 3.9422049522399902, 'learning_rate': 9.936568347605456e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3554, 'grad_norm': 4.29349422454834, 'learning_rate': 9.904852521408184e-05, 'epoch': 0.03}\n",
      "{'loss': 1.3709, 'grad_norm': 4.0807085037231445, 'learning_rate': 9.873136695210911e-05, 'epoch': 0.04}\n",
      "{'loss': 1.3018, 'grad_norm': 4.168679237365723, 'learning_rate': 9.841420869013638e-05, 'epoch': 0.05}\n",
      "{'loss': 1.2962, 'grad_norm': 4.147677421569824, 'learning_rate': 9.809705042816366e-05, 'epoch': 0.06}\n",
      "{'loss': 1.2682, 'grad_norm': 4.282947540283203, 'learning_rate': 9.777989216619093e-05, 'epoch': 0.07}\n",
      "{'loss': 1.2609, 'grad_norm': 3.9418632984161377, 'learning_rate': 9.746273390421821e-05, 'epoch': 0.08}\n",
      "{'loss': 1.2616, 'grad_norm': 3.8237521648406982, 'learning_rate': 9.714557564224549e-05, 'epoch': 0.09}\n",
      "{'loss': 1.2836, 'grad_norm': 3.9868547916412354, 'learning_rate': 9.682841738027276e-05, 'epoch': 0.1}\n",
      "{'loss': 1.267, 'grad_norm': 3.708913564682007, 'learning_rate': 9.651125911830003e-05, 'epoch': 0.1}\n",
      "{'loss': 1.1863, 'grad_norm': 3.2354745864868164, 'learning_rate': 9.619410085632731e-05, 'epoch': 0.11}\n",
      "{'loss': 1.2115, 'grad_norm': 4.003520488739014, 'learning_rate': 9.58769425943546e-05, 'epoch': 0.12}\n",
      "{'loss': 1.2303, 'grad_norm': 4.131709575653076, 'learning_rate': 9.555978433238186e-05, 'epoch': 0.13}\n",
      "{'loss': 1.1937, 'grad_norm': 4.937885284423828, 'learning_rate': 9.524262607040913e-05, 'epoch': 0.14}\n",
      "{'loss': 1.1571, 'grad_norm': 3.406914710998535, 'learning_rate': 9.492546780843641e-05, 'epoch': 0.15}\n",
      "{'loss': 1.2197, 'grad_norm': 2.8646321296691895, 'learning_rate': 9.46083095464637e-05, 'epoch': 0.16}\n",
      "{'loss': 1.2674, 'grad_norm': 5.238450527191162, 'learning_rate': 9.429749444973042e-05, 'epoch': 0.17}\n",
      "{'loss': 1.1666, 'grad_norm': 3.9476804733276367, 'learning_rate': 9.39803361877577e-05, 'epoch': 0.18}\n",
      "{'loss': 1.2115, 'grad_norm': 3.837080478668213, 'learning_rate': 9.366317792578497e-05, 'epoch': 0.19}\n",
      "{'loss': 1.1975, 'grad_norm': 5.164061546325684, 'learning_rate': 9.334601966381224e-05, 'epoch': 0.2}\n",
      "{'loss': 1.1535, 'grad_norm': 3.828110456466675, 'learning_rate': 9.302886140183951e-05, 'epoch': 0.21}\n",
      "{'loss': 1.1582, 'grad_norm': 4.349820613861084, 'learning_rate': 9.271170313986681e-05, 'epoch': 0.22}\n",
      "{'loss': 1.1549, 'grad_norm': 3.5192549228668213, 'learning_rate': 9.239454487789408e-05, 'epoch': 0.23}\n",
      "{'loss': 1.1361, 'grad_norm': 4.14235258102417, 'learning_rate': 9.207738661592134e-05, 'epoch': 0.24}\n",
      "{'loss': 1.1875, 'grad_norm': 3.2640914916992188, 'learning_rate': 9.176022835394863e-05, 'epoch': 0.25}\n",
      "{'loss': 1.1164, 'grad_norm': 3.811753749847412, 'learning_rate': 9.14430700919759e-05, 'epoch': 0.26}\n",
      "{'loss': 1.1181, 'grad_norm': 4.510441303253174, 'learning_rate': 9.112591183000318e-05, 'epoch': 0.27}\n",
      "{'loss': 1.1472, 'grad_norm': 4.081345558166504, 'learning_rate': 9.080875356803045e-05, 'epoch': 0.28}\n",
      "{'loss': 1.1469, 'grad_norm': 4.42120885848999, 'learning_rate': 9.049159530605773e-05, 'epoch': 0.29}\n",
      "{'loss': 1.1756, 'grad_norm': 4.1247053146362305, 'learning_rate': 9.0174437044085e-05, 'epoch': 0.29}\n",
      "{'loss': 1.1477, 'grad_norm': 4.365824222564697, 'learning_rate': 8.985727878211228e-05, 'epoch': 0.3}\n",
      "{'loss': 1.1502, 'grad_norm': 3.704568386077881, 'learning_rate': 8.954012052013956e-05, 'epoch': 0.31}\n",
      "{'loss': 1.1558, 'grad_norm': 4.442931175231934, 'learning_rate': 8.922296225816683e-05, 'epoch': 0.32}\n",
      "{'loss': 1.1209, 'grad_norm': 3.863223075866699, 'learning_rate': 8.89058039961941e-05, 'epoch': 0.33}\n",
      "{'loss': 1.1191, 'grad_norm': 3.388928174972534, 'learning_rate': 8.858864573422137e-05, 'epoch': 0.34}\n",
      "{'loss': 1.1161, 'grad_norm': 3.225069522857666, 'learning_rate': 8.827148747224867e-05, 'epoch': 0.35}\n",
      "{'loss': 1.1009, 'grad_norm': 4.109169006347656, 'learning_rate': 8.795432921027593e-05, 'epoch': 0.36}\n",
      "{'loss': 1.1011, 'grad_norm': 3.832617998123169, 'learning_rate': 8.76371709483032e-05, 'epoch': 0.37}\n",
      "{'loss': 1.1458, 'grad_norm': 3.5618488788604736, 'learning_rate': 8.732001268633049e-05, 'epoch': 0.38}\n",
      "{'loss': 1.1018, 'grad_norm': 4.261903762817383, 'learning_rate': 8.700285442435775e-05, 'epoch': 0.39}\n",
      "{'loss': 1.0976, 'grad_norm': 4.456055164337158, 'learning_rate': 8.668569616238504e-05, 'epoch': 0.4}\n",
      "{'loss': 1.1504, 'grad_norm': 3.1871984004974365, 'learning_rate': 8.63685379004123e-05, 'epoch': 0.41}\n",
      "{'loss': 1.091, 'grad_norm': 3.5255699157714844, 'learning_rate': 8.605137963843959e-05, 'epoch': 0.42}\n",
      "{'loss': 1.0904, 'grad_norm': 3.248372793197632, 'learning_rate': 8.573422137646686e-05, 'epoch': 0.43}\n",
      "{'loss': 1.1446, 'grad_norm': 4.095454216003418, 'learning_rate': 8.541706311449414e-05, 'epoch': 0.44}\n",
      "{'loss': 1.1818, 'grad_norm': 3.436781883239746, 'learning_rate': 8.509990485252142e-05, 'epoch': 0.45}\n",
      "{'loss': 1.0601, 'grad_norm': 4.953577995300293, 'learning_rate': 8.478274659054869e-05, 'epoch': 0.46}\n",
      "{'loss': 1.1037, 'grad_norm': 3.2231833934783936, 'learning_rate': 8.446558832857596e-05, 'epoch': 0.47}\n",
      "{'loss': 1.0642, 'grad_norm': 3.2503204345703125, 'learning_rate': 8.414843006660323e-05, 'epoch': 0.48}\n",
      "{'loss': 1.0468, 'grad_norm': 3.4586119651794434, 'learning_rate': 8.383127180463052e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0961, 'grad_norm': 3.4712040424346924, 'learning_rate': 8.351411354265779e-05, 'epoch': 0.49}\n",
      "{'loss': 1.0957, 'grad_norm': 3.5523738861083984, 'learning_rate': 8.319695528068506e-05, 'epoch': 0.5}\n",
      "{'loss': 1.084, 'grad_norm': 2.822319507598877, 'learning_rate': 8.287979701871234e-05, 'epoch': 0.51}\n",
      "{'loss': 1.0267, 'grad_norm': 3.5313334465026855, 'learning_rate': 8.256263875673961e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0797, 'grad_norm': 2.6931850910186768, 'learning_rate': 8.22454804947669e-05, 'epoch': 0.53}\n",
      "{'loss': 1.074, 'grad_norm': 4.376208782196045, 'learning_rate': 8.192832223279416e-05, 'epoch': 0.54}\n",
      "{'loss': 1.0281, 'grad_norm': 4.792250156402588, 'learning_rate': 8.161116397082145e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0865, 'grad_norm': 3.58547043800354, 'learning_rate': 8.129400570884872e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0333, 'grad_norm': 3.112736225128174, 'learning_rate': 8.0976847446876e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0746, 'grad_norm': 3.3447136878967285, 'learning_rate': 8.065968918490328e-05, 'epoch': 0.58}\n",
      "{'loss': 1.0587, 'grad_norm': 3.2216873168945312, 'learning_rate': 8.034887408817e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0529, 'grad_norm': 3.404710054397583, 'learning_rate': 8.003171582619727e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0447, 'grad_norm': 4.288875579833984, 'learning_rate': 7.971455756422456e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0868, 'grad_norm': 3.164511203765869, 'learning_rate': 7.939739930225183e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0497, 'grad_norm': 3.7179999351501465, 'learning_rate': 7.908024104027911e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0217, 'grad_norm': 3.614881992340088, 'learning_rate': 7.876308277830638e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0236, 'grad_norm': 2.879349708557129, 'learning_rate': 7.844592451633366e-05, 'epoch': 0.65}\n",
      "{'loss': 1.0455, 'grad_norm': 3.399946928024292, 'learning_rate': 7.812876625436093e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9982, 'grad_norm': 3.4282500743865967, 'learning_rate': 7.78116079923882e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0635, 'grad_norm': 2.908236265182495, 'learning_rate': 7.749444973041549e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0038, 'grad_norm': 4.4789862632751465, 'learning_rate': 7.717729146844276e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0596, 'grad_norm': 3.176464557647705, 'learning_rate': 7.686013320647003e-05, 'epoch': 0.69}\n",
      "{'loss': 0.9773, 'grad_norm': 3.0398447513580322, 'learning_rate': 7.65429749444973e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0541, 'grad_norm': 3.347963571548462, 'learning_rate': 7.622581668252458e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0275, 'grad_norm': 4.137833118438721, 'learning_rate': 7.590865842055186e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0049, 'grad_norm': 3.5660641193389893, 'learning_rate': 7.559150015857913e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0138, 'grad_norm': 3.630953550338745, 'learning_rate': 7.527434189660642e-05, 'epoch': 0.74}\n",
      "{'loss': 0.9691, 'grad_norm': 4.805905342102051, 'learning_rate': 7.495718363463368e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9932, 'grad_norm': 4.588711738586426, 'learning_rate': 7.464002537266097e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0127, 'grad_norm': 3.136706829071045, 'learning_rate': 7.432286711068824e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0019, 'grad_norm': 3.341554880142212, 'learning_rate': 7.400570884871552e-05, 'epoch': 0.78}\n",
      "{'loss': 1.0212, 'grad_norm': 3.7265827655792236, 'learning_rate': 7.368855058674279e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0377, 'grad_norm': 3.2796876430511475, 'learning_rate': 7.337139232477006e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0052, 'grad_norm': 3.92486310005188, 'learning_rate': 7.305423406279734e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9956, 'grad_norm': 3.3764870166778564, 'learning_rate': 7.273707580082462e-05, 'epoch': 0.82}\n",
      "{'loss': 1.0143, 'grad_norm': 2.7492969036102295, 'learning_rate': 7.241991753885189e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0227, 'grad_norm': 3.2849395275115967, 'learning_rate': 7.210275927687916e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9947, 'grad_norm': 3.44864559173584, 'learning_rate': 7.178560101490644e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9939, 'grad_norm': 3.6427440643310547, 'learning_rate': 7.146844275293372e-05, 'epoch': 0.86}\n",
      "{'loss': 1.007, 'grad_norm': 3.4471704959869385, 'learning_rate': 7.115128449096099e-05, 'epoch': 0.87}\n",
      "{'loss': 0.9957, 'grad_norm': 4.047391891479492, 'learning_rate': 7.083412622898827e-05, 'epoch': 0.88}\n",
      "{'loss': 0.9936, 'grad_norm': 2.9805073738098145, 'learning_rate': 7.051696796701554e-05, 'epoch': 0.88}\n",
      "{'loss': 0.9501, 'grad_norm': 2.979877233505249, 'learning_rate': 7.019980970504281e-05, 'epoch': 0.89}\n",
      "{'loss': 0.958, 'grad_norm': 3.680532932281494, 'learning_rate': 6.98826514430701e-05, 'epoch': 0.9}\n",
      "{'loss': 0.9967, 'grad_norm': 2.5902788639068604, 'learning_rate': 6.956549318109738e-05, 'epoch': 0.91}\n",
      "{'loss': 0.9761, 'grad_norm': 2.879917860031128, 'learning_rate': 6.924833491912465e-05, 'epoch': 0.92}\n",
      "{'loss': 0.9699, 'grad_norm': 8.16826343536377, 'learning_rate': 6.893117665715191e-05, 'epoch': 0.93}\n",
      "{'loss': 0.962, 'grad_norm': 3.73482084274292, 'learning_rate': 6.86140183951792e-05, 'epoch': 0.94}\n",
      "{'loss': 0.9856, 'grad_norm': 4.187489986419678, 'learning_rate': 6.829686013320648e-05, 'epoch': 0.95}\n",
      "{'loss': 0.9572, 'grad_norm': 3.3293650150299072, 'learning_rate': 6.797970187123375e-05, 'epoch': 0.96}\n",
      "{'loss': 0.953, 'grad_norm': 3.5621750354766846, 'learning_rate': 6.766254360926102e-05, 'epoch': 0.97}\n",
      "{'loss': 0.9266, 'grad_norm': 4.332016468048096, 'learning_rate': 6.73453853472883e-05, 'epoch': 0.98}\n",
      "{'loss': 1.0049, 'grad_norm': 4.111320495605469, 'learning_rate': 6.702822708531558e-05, 'epoch': 0.99}\n",
      "{'loss': 1.0058, 'grad_norm': 2.6229865550994873, 'learning_rate': 6.671106882334285e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12ba175118c4744834f64a11063f83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      2\u001b[0m     model,\n\u001b[0;32m      3\u001b[0m     args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer.py:2043\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2041\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[1;32m-> 2043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2044\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2045\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2046\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2047\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2048\u001b[0m     )\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2050\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer.py:2487\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2491\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer.py:2915\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2913\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2915\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m   2918\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer.py:2872\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 2872\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys_for_eval)\n\u001b[0;32m   2873\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2875\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:180\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys, metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix)\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[0;32m   3869\u001b[0m     eval_dataloader,\n\u001b[0;32m   3870\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3871\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[0;32m   3872\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[0;32m   3873\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3874\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[0;32m   3875\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[0;32m   3876\u001b[0m )\n\u001b[0;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer.py:4061\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4058\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4060\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4061\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys)\n\u001b[0;32m   4062\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4063\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:310\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    306\u001b[0m ):\n\u001b[0;32m    307\u001b[0m     generation_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    308\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    309\u001b[0m     }\n\u001b[1;32m--> 310\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\generation\\utils.py:2078\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2070\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2071\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2072\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2073\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2074\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2075\u001b[0m     )\n\u001b[0;32m   2077\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2078\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2079\u001b[0m         input_ids,\n\u001b[0;32m   2080\u001b[0m         beam_scorer,\n\u001b[0;32m   2081\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2082\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2083\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2084\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2085\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2086\u001b[0m     )\n\u001b[0;32m   2088\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2089\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2090\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2091\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2092\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2098\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2099\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\generation\\utils.py:3314\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3311\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m next_tokens \u001b[38;5;241m%\u001b[39m vocab_size\n\u001b[0;32m   3313\u001b[0m \u001b[38;5;66;03m# stateless\u001b[39;00m\n\u001b[1;32m-> 3314\u001b[0m beam_outputs \u001b[38;5;241m=\u001b[39m beam_scorer\u001b[38;5;241m.\u001b[39mprocess(\n\u001b[0;32m   3315\u001b[0m     input_ids,\n\u001b[0;32m   3316\u001b[0m     next_token_scores,\n\u001b[0;32m   3317\u001b[0m     next_tokens,\n\u001b[0;32m   3318\u001b[0m     next_indices,\n\u001b[0;32m   3319\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mpad_token_id,\n\u001b[0;32m   3320\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39meos_token_id,\n\u001b[0;32m   3321\u001b[0m     beam_indices\u001b[38;5;241m=\u001b[39mbeam_indices,\n\u001b[0;32m   3322\u001b[0m     decoder_prompt_len\u001b[38;5;241m=\u001b[39mdecoder_prompt_len,\n\u001b[0;32m   3323\u001b[0m )\n\u001b[0;32m   3325\u001b[0m beam_scores \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3326\u001b[0m beam_next_tokens \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\generation\\beam_search.py:256\u001b[0m, in \u001b[0;36mBeamSearchScorer.process\u001b[1;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, decoder_prompt_len)\u001b[0m\n\u001b[0;32m    254\u001b[0m batch_group_idx \u001b[38;5;241m=\u001b[39m batch_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beam_groups \u001b[38;5;241m+\u001b[39m group_index\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done[batch_group_idx]:\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beams \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_hyps[batch_group_idx]):\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch can only be done if at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m beams have been generated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\generation\\beam_search.py:948\u001b[0m, in \u001b[0;36mBeamHypotheses.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `do_early_stopping` is set to a string, `max_length` must be defined. Ensure it is passed to the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    945\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m BeamScorer class instance at initialization time.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    946\u001b[0m         )\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    949\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;124;03m    Number of hypotheses in the list.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeams)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
