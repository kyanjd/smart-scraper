{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for kde4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/kde4\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 210173\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 168138\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'translation'],\n",
      "        num_rows: 42035\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.8, seed=20)\n",
    "\n",
    "print(split_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'fr'],\n",
      "        num_rows: 168138\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['en', 'fr'],\n",
      "        num_rows: 42035\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def flatten_translation(examples):\n",
    "    return {\n",
    "        \"en\": [ex[\"en\"] for ex in examples[\"translation\"]],\n",
    "        \"fr\": [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    }\n",
    "\n",
    "equivalent_datasets = split_datasets.map(flatten_translation, batched=True, remove_columns=[\"id\", \"translation\"])\n",
    "\n",
    "print(equivalent_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kyanj\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-fr. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1232, 13572, 7823, 9, 0], 'attention_mask': [1, 1, 1, 1, 1], 'labels': [22181, 10691, 412, 9, 1232, 21332, 0]}\n",
      "Web Shortcuts</s>\n",
      "[1232, 13572, 7823, 9, 0]\n",
      "Raccourcis WebComment</s>\n"
     ]
    }
   ],
   "source": [
    "en_sentence = equivalent_datasets[\"train\"][\"en\"][0]\n",
    "fr_sentence = equivalent_datasets[\"train\"][\"fr\"][0]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=fr_sentence)\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))\n",
    "print(tokenizer.encode(en_sentence))\n",
    "print(tokenizer.decode(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'text_targets': ['Raccourcis WebComment', 'Téléchargez le depuis la section Fichiers (http: / /download. gna. org/ kvpnc/).', 'Texte %1', \"K3b nécessite l'installation du programme « & #160; mkisofs & #160; » en version 1.14 (ou supérieure). Les versions antérieures posent des problèmes lors de la création de projets de données.\"]} not recognized.\n",
      "Keyword arguments {'text_targets': ['Raccourcis WebComment', 'Téléchargez le depuis la section Fichiers (http: / /download. gna. org/ kvpnc/).', 'Texte %1', \"K3b nécessite l'installation du programme « & #160; mkisofs & #160; » en version 1.14 (ou supérieure). Les versions antérieures posent des problèmes lors de la création de projets de données.\"]} not recognized.\n",
      "Keyword arguments {'text_targets': ['Raccourcis WebComment', 'Téléchargez le depuis la section Fichiers (http: / /download. gna. org/ kvpnc/).', 'Texte %1', \"K3b nécessite l'installation du programme « & #160; mkisofs & #160; » en version 1.14 (ou supérieure). Les versions antérieures posent des problèmes lors de la création de projets de données.\"]} not recognized.\n",
      "Keyword arguments {'text_targets': ['Raccourcis WebComment', 'Téléchargez le depuis la section Fichiers (http: / /download. gna. org/ kvpnc/).', 'Texte %1', \"K3b nécessite l'installation du programme « & #160; mkisofs & #160; » en version 1.14 (ou supérieure). Les versions antérieures posent des problèmes lors de la création de projets de données.\"]} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1232, 13572, 7823, 9, 0], [35, 723, 647, 373, 45, 928, 71, 37, 4012, 9, 37, 583, 583, 3390, 3, 49, 19015, 3, 57, 309, 74, 1013, 74, 2635, 973, 529, 364, 222, 50, 3, 0], [45629, 0], [526, 602, 226, 895, 71, 1187, 251, 5049, 9, 2368, 9, 1226, 6662, 6426, 34144, 5056, 202, 8101, 1366, 288, 4933, 499, 1013, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"en\"]\n",
    "    targets = examples[\"fr\"]\n",
    "    model_inputs = tokenizer(inputs, text_targets=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "data_check = equivalent_datasets[\"train\"][0:4]\n",
    "tokenized_datasets_eq = equivalent_datasets.map(preprocess_function, batched=True, remove_columns=equivalent_datasets[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'text_targets': ['Raccourcis WebComment', 'Téléchargez le depuis la section Fichiers (http: / /download. gna. org/ kvpnc/).', 'Texte %1', \"K3b nécessite l'installation du programme « & #160; mkisofs & #160; » en version 1.14 (ou supérieure). Les versions antérieures posent des problèmes lors de la création de projets de données.\"]} not recognized.\n",
      "Keyword arguments {'text_targets': ['Raccourcis WebComment', 'Téléchargez le depuis la section Fichiers (http: / /download. gna. org/ kvpnc/).', 'Texte %1', \"K3b nécessite l'installation du programme « & #160; mkisofs & #160; » en version 1.14 (ou supérieure). Les versions antérieures posent des problèmes lors de la création de projets de données.\"]} not recognized.\n",
      "Keyword arguments {'text_targets': ['Raccourcis WebComment', 'Téléchargez le depuis la section Fichiers (http: / /download. gna. org/ kvpnc/).', 'Texte %1', \"K3b nécessite l'installation du programme « & #160; mkisofs & #160; » en version 1.14 (ou supérieure). Les versions antérieures posent des problèmes lors de la création de projets de données.\"]} not recognized.\n",
      "Keyword arguments {'text_targets': ['Raccourcis WebComment', 'Téléchargez le depuis la section Fichiers (http: / /download. gna. org/ kvpnc/).', 'Texte %1', \"K3b nécessite l'installation du programme « & #160; mkisofs & #160; » en version 1.14 (ou supérieure). Les versions antérieures posent des problèmes lors de la création de projets de données.\"]} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1232, 13572, 7823, 9, 0], [35, 723, 647, 373, 45, 928, 71, 37, 4012, 9, 37, 583, 583, 3390, 3, 49, 19015, 3, 57, 309, 74, 1013, 74, 2635, 973, 529, 364, 222, 50, 3, 0], [45629, 0], [526, 602, 226, 895, 71, 1187, 251, 5049, 9, 2368, 9, 1226, 6662, 6426, 34144, 5056, 202, 8101, 1366, 288, 4933, 499, 1013, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "def preprocess_function2(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_targets=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "data_check = split_datasets[\"train\"][0:4]\n",
    "print(preprocess_function2(data_check))\n",
    "tokenized_datasets = split_datasets.map(preprocess_function2, batched=True, remove_columns=split_datasets[\"train\"].column_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
