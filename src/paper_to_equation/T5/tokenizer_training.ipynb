{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "from transformers import T5Tokenizer\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer, Regex\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import Metaspace, Split, WhitespaceSplit\n",
    "from huggingface_hub import login\n",
    "\n",
    "import re\n",
    "import src.paper_to_equation.Generation.Equation_BaseDataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(src.paper_to_equation.Generation.Equation_BaseDataset)\n",
    "from src.paper_to_equation.Generation.Equation_BaseDataset import BaseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the default tokenizer is unsuitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'h', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', 'mm', 'l', ':', 'm', 'o', '>', '=', '<', '/', 'mm', 'l', ':', 'm', 'o', '>', '▁', '<', 'mm', 'l', ':', 'm', 'row', '>', '▁', '<', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'h', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'c', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', '/', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', 'mm', 'l', ':', 'm', 'o', '>', '+', '<', '/', 'mm', 'l', ':', 'm', 'o', '>', '▁', '<', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'h', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'g', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', '/', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', '/', 'mm', 'l', ':', 'm', 'row', '>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "mml = \"\"\"\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mo>=</mml:mo>\n",
    "<mml:mrow>\n",
    "<mml:msub>\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mi>c</mml:mi>\n",
    "</mml:msub>\n",
    "<mml:mo>+</mml:mo>\n",
    "<mml:msub>\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mi>g</mml:mi>\n",
    "</mml:msub>\n",
    "</mml:mrow>\n",
    "  \"\"\"\n",
    "\n",
    "py = \"\"\"\n",
    "h = Symbol('h')\n",
    "h_g = Symbol('h_g')\n",
    "h_c = Symbol('h_c')\n",
    "e = Eq(h, h_g + h_c)\"\"\"\n",
    "\n",
    "tokens = tokenizer.tokenize(mml)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerDataset(BaseDataset):\n",
    "    def __init__(self, num):\n",
    "        super().__init__(num)\n",
    "\n",
    "    def get_columns(self):\n",
    "        return [\"mathml\", \"python\"]\n",
    "\n",
    "    def map_atomic_tokens(self, dataset):\n",
    "\n",
    "        tag_map = {\"<mml:mo>\": \"<MO>\", \"</mml:mo>\": \"</MO>\",\n",
    "                   \"<mml:mi>\": \"<MI>\", \"</mml:mi>\": \"</MI>\",\n",
    "                   \"<mml:msub>\": \"<MSUB>\", \"</mml:msub>\": \"</MSUB>\",\n",
    "                   \"<mml:msup>\": \"<MSUP>\", \"</mml:msup>\": \"</MSUP>\",\n",
    "                   \"<mml:mrow>\": \"<MROW>\", \"</mml:mrow>\": \"</MROW>\", \n",
    "                   \"<mml:mfrac>\": \"<MFRAC>\", \"</mml:mfrac>\": \"</MFRAC>\"}\n",
    "        \n",
    "        for entry in dataset:\n",
    "            mathml = entry[\"mathml\"]\n",
    "            for tag, token in tag_map.items():\n",
    "                mathml = mathml.replace(tag, token)\n",
    "            entry[\"mathml\"] = mathml\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def get_tag_list(self):\n",
    "        return [\"<mml:mo>\", \"</mml:mo>\",\n",
    "                \"<mml:mi>\", \"</mml:mi>\",\n",
    "                \"<mml:msub>\", \"</mml:msub>\",\n",
    "                \"<mml:msup>\", \"</mml:msup>\",\n",
    "                \"<mml:mrow>\", \"</mml:mrow>\",\n",
    "                \"<mml:mfrac>\", \"</mml:mfrac>\",\n",
    "                \"<mml:mtext>\", \"</mml:mtext>\"]\n",
    "    \n",
    "    def extract_tags(self, data):\n",
    "        tags = set()\n",
    "        for entry in data:\n",
    "            mathml = entry[\"mathml\"]\n",
    "            for tag in mathml:\n",
    "                if tag not in tags:\n",
    "                    tags.append(tag)\n",
    "        return tags\n",
    "        \n",
    "    def data_iterator(self, batch_size):\n",
    "        columns = self.get_columns()\n",
    "        for i in range(0, len(self.dataset), batch_size):\n",
    "            yield [f\"{data[columns[0]]} {data[columns[1]]} \"for data in self.dataset[i:i+batch_size]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class method (unsuccessful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No constructor defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m\n\u001b[0;32m     45\u001b[0m                 splits\u001b[38;5;241m.\u001b[39mappend((text[last_end:], last_end \u001b[38;5;241m+\u001b[39m offset))\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m splits\n\u001b[1;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m MathMLPyTokenizer()\n\u001b[0;32m     51\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(mml)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: No constructor defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())                  \n",
    "\n",
    "class MathMLPyTokenizer(pre_tokenizers.PreTokenizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pre_tokenize(self, data):\n",
    "        \n",
    "        patterns = [\n",
    "            # HTML/XML tags\n",
    "            r'<[^>]+>',\n",
    "            # Python string literals\n",
    "            r'\"\"\"[^\"]*\"\"\"', r\"'''[^']*'''\", r'\"[^\"]*\"', r\"'[^']*'\",\n",
    "            # Python keywords and operators\n",
    "            r'\\bdef\\b', r'\\bclass\\b', r'\\bfor\\b', r'\\bwhile\\b', r'\\bif\\b', r'\\belif\\b', r'\\belse\\b',\n",
    "            r'\\breturn\\b', r'\\bimport\\b', r'\\bfrom\\b', r'\\bas\\b', r'\\bwith\\b', r'\\btry\\b', r'\\bexcept\\b',\n",
    "            # Common Python syntax elements\n",
    "            r'==', r'!=', r'<=', r'>=', r'\\+=', r'-=', r'\\*=', r'/=', \n",
    "            r'=>', r'->',  # Function type hints and lambdas\n",
    "            r'\\bSymbol\\b', r'\\bEq\\b', r'\\bexp\\b', r'\\bsin\\b', r'\\bcos\\b', r'\\btan\\b', r'\\bdiff\\b',\n",
    "            \n",
    "            # Indentation (important for Python)\n",
    "            r'^\\s+'\n",
    "        ]\n",
    "\n",
    "        combined_pattern = '|'.join(f'({p})' for p in patterns)\n",
    "        regex = re.compile(combined_pattern, re.MULTILINE)\n",
    "        \n",
    "        splits = []\n",
    "        for text, offset in data:\n",
    "            last_end = 0\n",
    "            for match in regex.finditer(text):\n",
    "                start, end = match.span()\n",
    "                \n",
    "                if start > last_end:\n",
    "                    # Add text before the special token\n",
    "                    splits.append((text[last_end:start], last_end + offset))\n",
    "                \n",
    "                # Add the special token as a whole\n",
    "                splits.append((text[start:end], start + offset))\n",
    "                last_end = end\n",
    "            \n",
    "            if last_end < len(text):\n",
    "                # Add remaining text\n",
    "                splits.append((text[last_end:], last_end + offset))\n",
    "                \n",
    "        return splits\n",
    "\n",
    "x = MathMLPyTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(mml)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset creation timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 100%|██████████| 10000/10000 [01:39<00:00, 100.33it/s]\n",
      "Generating dataset:  92%|█████████▏| 9223/10000 [01:28<00:07, 104.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1:  99.67798781394958\n",
      "Time 2:  90.23344993591309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start1 = time.time()\n",
    "    td1 = TokenizerDataset(10000)\n",
    "    td1.create_dataset()\n",
    "    end1 = time.time()\n",
    "\n",
    "    start2 = time.time()\n",
    "    td2 = TokenizerDataset(10000)\n",
    "    td2.create_dataset_mthread()\n",
    "    end2 = time.time()\n",
    "\n",
    "    print(\"Time 1: \", end1 - start1)\n",
    "    print(\"Time 2: \", end2 - start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "td = TokenizerDataset(10000)\n",
    "td.create_dataset()\n",
    "print(len(td.dataset))\n",
    "data_iterator = td.data_iterator(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base tokenizer (BPE, WordPiece, or Unigram)\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Define patterns we want to protect during tokenization\n",
    "text_patterns = [\n",
    "            # HTML/XML tags\n",
    "            r'<[^>]+>',\n",
    "            # Python string literals\n",
    "            r'\"\"\"[^\"]*\"\"\"', r\"'''[^']*'''\", r'\"[^\"]*\"', r\"'[^']*'\",\n",
    "            # Python keywords and operators\n",
    "            r'\\bdef\\b', r'\\bclass\\b', r'\\bfor\\b', r'\\bwhile\\b', r'\\bif\\b', r'\\belif\\b', r'\\belse\\b',\n",
    "            r'\\breturn\\b', r'\\bimport\\b', r'\\bfrom\\b', r'\\bas\\b', r'\\bwith\\b', r'\\btry\\b', r'\\bexcept\\b',\n",
    "            # Common Python syntax elements\n",
    "            r'==', r'!=', r'<=', r'>=', r'\\+=', r'-=', r'\\*=', r'/=', \n",
    "            r'=>', r'->',  # Function type hints and lambdas\n",
    "            r'\\bSymbol\\b', r'\\bEq\\b', r'\\bexp\\b', r'\\bsin\\b', r'\\bcos\\b', r'\\btan\\b', r'\\bdiff\\b', # MathML\n",
    "            # Indentation (important for Python)\n",
    "            r'^\\s+',\n",
    "            r'\\s+' # Whitespace\n",
    "        ]\n",
    "\n",
    "# Set up the pre-tokenizer using Split with pattern\n",
    "pattern = '|'.join(text_patterns)\n",
    "split_pre_tokenizer = Split(pattern=Regex(pattern), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = split_pre_tokenizer\n",
    "\n",
    "# Set up other components \n",
    "tokenizer.normalizer = Sequence([NFKC()])\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "tag_list = td.get_tag_list()\n",
    "special_tokens = [\"[PAD]\", \"[BOS]\", \"[EOS]\", \"[UNK]\"] + tag_list\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(iterator=data_iterator, trainer=trainer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\kyanj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_login_key = os.environ.get(\"HF_LOGIN_KEY\")\n",
    "login(token=hf_login_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"Tokenizer_Files/mathml-py-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "def my_function(x, y): return x + y\n",
    "class MyClass:\n",
    "    def method(self): pass\n",
    "import numpy as np\n",
    "<math><msup><mi>x</mi><mn>2</mn></msup></math>\n",
    "\"\"\"\n",
    "\n",
    "check = tokenizer.pre_tokenizer.pre_tokenize_str(mml)\n",
    "print(check)\n",
    "output = tokenizer.encode(test_text)\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '<mml:mi>', 'h', '</mml:mi>', '\\n', '<mml:mo>', '=', '</mml:mo>', '\\n', '<mml:mrow>', '\\n', '<mml:msub>', '\\n', '<mml:mi>', 'h', '</mml:mi>', '\\n', '<mml:mi>', 'c', '</mml:mi>', '\\n', '</mml:msub>', '\\n', '<mml:mo>', '+', '</mml:mo>', '\\n', '<mml:msub>', '\\n', '<mml:mi>', 'h', '</mml:mi>', '\\n', '<mml:mi>', 'g', '</mml:mi>', '\\n', '</mml:msub>', '\\n', '</mml:mrow>', '\\n', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "output_mathml = tokenizer.encode(td.dataset[0][\"mathml\"])\n",
    "output_py = tokenizer.encode(td.dataset[0][\"python\"])\n",
    "# print(output_py.tokens)\n",
    "# print(output_py.tokens)\n",
    "# print(output_py.ids)\n",
    "# print(tokenizer.decode(output_py.ids))\n",
    "output_test = tokenizer.encode(mml)\n",
    "print(output_test.tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
