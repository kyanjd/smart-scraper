{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import importlib\n",
    "from transformers import T5Tokenizer\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer, Regex\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import Metaspace, Split, WhitespaceSplit\n",
    "\n",
    "import re\n",
    "import src.paper_to_equation.Generation.Equation_BaseDataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(src.paper_to_equation.Generation.Equation_BaseDataset)\n",
    "from src.paper_to_equation.Generation.Equation_BaseDataset import BaseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the default tokenizer is unsuitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'h', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', 'mm', 'l', ':', 'm', 'o', '>', '=', '<', '/', 'mm', 'l', ':', 'm', 'o', '>', '▁', '<', 'mm', 'l', ':', 'm', 'row', '>', '▁', '<', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'h', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'c', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', '/', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', 'mm', 'l', ':', 'm', 'o', '>', '+', '<', '/', 'mm', 'l', ':', 'm', 'o', '>', '▁', '<', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'h', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'g', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', '/', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', '/', 'mm', 'l', ':', 'm', 'row', '>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "mml = \"\"\"\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mo>=</mml:mo>\n",
    "<mml:mrow>\n",
    "<mml:msub>\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mi>c</mml:mi>\n",
    "</mml:msub>\n",
    "<mml:mo>+</mml:mo>\n",
    "<mml:msub>\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mi>g</mml:mi>\n",
    "</mml:msub>\n",
    "</mml:mrow>\n",
    "  \"\"\"\n",
    "\n",
    "py = \"\"\"\n",
    "h = Symbol('h')\n",
    "h_g = Symbol('h_g')\n",
    "h_c = Symbol('h_c')\n",
    "e = Eq(h, h_g + h_c)\"\"\"\n",
    "\n",
    "tokens = tokenizer.tokenize(mml)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerDataset(BaseDataset):\n",
    "    def __init__(self, num, filepath):\n",
    "        super().__init__(num, filepath)\n",
    "\n",
    "    def get_columns(self):\n",
    "        return [\"mathml\", \"python\"]\n",
    "\n",
    "    def map_atomic_tokens(self, dataset):\n",
    "\n",
    "        tag_map = {\"<mml:mo>\": \"<MO>\", \"</mml:mo>\": \"</MO>\",\n",
    "                   \"<mml:mi>\": \"<MI>\", \"</mml:mi>\": \"</MI>\",\n",
    "                   \"<mml:msub>\": \"<MSUB>\", \"</mml:msub>\": \"</MSUB>\",\n",
    "                   \"<mml:msup>\": \"<MSUP>\", \"</mml:msup>\": \"</MSUP>\",\n",
    "                   \"<mml:mrow>\": \"<MROW>\", \"</mml:mrow>\": \"</MROW>\", \n",
    "                   \"<mml:mfrac>\": \"<MFRAC>\", \"</mml:mfrac>\": \"</MFRAC>\"}\n",
    "        \n",
    "        for entry in dataset:\n",
    "            mathml = entry[\"mathml\"]\n",
    "            for tag, token in tag_map.items():\n",
    "                mathml = mathml.replace(tag, token)\n",
    "            entry[\"mathml\"] = mathml\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def json_to_dataset(self):\n",
    "        with open(self.filepath) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def get_tag_list():\n",
    "        return [\"<mml:mo>\", \"</mml:mo>\",\n",
    "                \"<mml:mi>\", \"</mml:mi>\",\n",
    "                \"<mml:msub>\", \"</mml:msub>\",\n",
    "                \"<mml:msup>\", \"</mml:msup>\",\n",
    "                \"<mml:mrow>\", \"</mml:mrow>\",\n",
    "                \"<mml:mfrac>\", \"</mml:mfrac>\"\n",
    "                \"<mml:mtext>\", \"</mml:mtext>\"]\n",
    "    \n",
    "    \n",
    "    def extract_tags(self, data):\n",
    "        tags = set()\n",
    "        for entry in data:\n",
    "            mathml = entry[\"mathml\"]\n",
    "            for tag in mathml:\n",
    "                if tag not in tags:\n",
    "                    tags.append(tag)\n",
    "        return tags\n",
    "        \n",
    "    def data_iterator(self, data, batch_size):\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            yield [sample[\"mathml\"] for sample in data[i:i+batch_size]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<mml:mi>h</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:msup>\\n<mml:mtext>exp</mml:mtext>\\n<mml:mrow>\\n<mml:mrow>\\n<mml:mo>-</mml:mo>\\n<mml:mi>Ε</mml:mi>\\n</mml:mrow>\\n<mml:mo>+</mml:mo>\\n<mml:mi>ν</mml:mi>\\n</mml:mrow>\\n</mml:msup>\\n<mml:mo>-</mml:mo>\\n<mml:mrow>\\n<mml:mi>sin</mml:mi>\\n<mml:mfenced>\\n<mml:msub>\\n<mml:mi>f</mml:mi>\\n<mml:mrow>\\n<mml:mi>t</mml:mi>\\n<mml:mi>S</mml:mi>\\n</mml:mrow>\\n</mml:msub>\\n</mml:mfenced>\\n</mml:mrow>\\n<mml:mo>+</mml:mo>\\n<mml:mrow>\\n<mml:mi>tan</mml:mi>\\n<mml:mfenced>\\n<mml:msub>\\n<mml:mi>ο</mml:mi>\\n<mml:mrow>\\n<mml:mi>T</mml:mi>\\n<mml:mi>λ</mml:mi>\\n</mml:mrow>\\n</mml:msub>\\n</mml:mfenced>\\n</mml:mrow>\\n</mml:mrow>', '<mml:mi>ν</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:munderover>\\n<mml:mo>∑</mml:mo>\\n<mml:mrow>\\n<mml:mi>κ</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mn>9</mml:mn>\\n</mml:mrow>\\n<mml:msub>\\n<mml:mi>y</mml:mi>\\n<mml:mi>Δ</mml:mi>\\n</mml:msub>\\n</mml:munderover>\\n<mml:mfenced>\\n<mml:mrow>\\n<mml:msqrt>\\n<mml:mrow>\\n<mml:mi>Λ</mml:mi>\\n<mml:mo>-</mml:mo>\\n<mml:mn>7</mml:mn>\\n</mml:mrow>\\n</mml:msqrt>\\n<mml:mo>+</mml:mo>\\n<mml:msup>\\n<mml:mtext>exp</mml:mtext>\\n<mml:mi>k</mml:mi>\\n</mml:msup>\\n</mml:mrow>\\n</mml:mfenced>\\n</mml:mrow>', '<mml:msub>\\n<mml:mi>G</mml:mi>\\n<mml:mi>θ</mml:mi>\\n</mml:msub>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:mfrac>\\n<mml:mrow>\\n<mml:mi>sin</mml:mi>\\n<mml:mfenced>\\n<mml:mi>y</mml:mi>\\n</mml:mfenced>\\n</mml:mrow>\\n<mml:mrow>\\n<mml:mi>cos</mml:mi>\\n<mml:mfenced>\\n<mml:mrow>\\n<mml:mi>q</mml:mi>\\n<mml:mo>-</mml:mo>\\n<mml:mn>8</mml:mn>\\n</mml:mrow>\\n</mml:mfenced>\\n</mml:mrow>\\n</mml:mfrac>\\n</mml:mrow>', '<mml:mi>O</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:munderover>\\n<mml:mo>∑</mml:mo>\\n<mml:mrow>\\n<mml:msub>\\n<mml:mi>h</mml:mi>\\n<mml:mi>w</mml:mi>\\n</mml:msub>\\n<mml:mo>=</mml:mo>\\n<mml:mn>8</mml:mn>\\n</mml:mrow>\\n<mml:msub>\\n<mml:mi>Ι</mml:mi>\\n<mml:mrow>\\n<mml:mi>6</mml:mi>\\n<mml:mi>Π</mml:mi>\\n<mml:mi>θ</mml:mi>\\n</mml:mrow>\\n</mml:msub>\\n</mml:munderover>\\n<mml:mfenced>\\n<mml:mrow>\\n<mml:mi>Π</mml:mi>\\n<mml:mo>+</mml:mo>\\n<mml:mrow>\\n<mml:mi>log</mml:mi>\\n<mml:mfenced>\\n<mml:msub>\\n<mml:mi>θ</mml:mi>\\n<mml:mrow>\\n<mml:mi>ι</mml:mi>\\n<mml:mi>β</mml:mi>\\n<mml:mi>Τ</mml:mi>\\n</mml:mrow>\\n</mml:msub>\\n</mml:mfenced>\\n</mml:mrow>\\n<mml:mo>+</mml:mo>\\n<mml:mrow>\\n<mml:mi>sin</mml:mi>\\n<mml:mfenced>\\n<mml:mi>ν</mml:mi>\\n</mml:mfenced>\\n</mml:mrow>\\n</mml:mrow>\\n</mml:mfenced>\\n</mml:mrow>', '<mml:mi>κ</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:munderover>\\n<mml:mo>∑</mml:mo>\\n<mml:mrow>\\n<mml:msub>\\n<mml:mi>φ</mml:mi>\\n<mml:mrow>\\n<mml:mi>ρ</mml:mi>\\n<mml:mi>α</mml:mi>\\n</mml:mrow>\\n</mml:msub>\\n<mml:mo>=</mml:mo>\\n<mml:mn>7</mml:mn>\\n</mml:mrow>\\n<mml:mi>c</mml:mi>\\n</mml:munderover>\\n<mml:mfenced>\\n<mml:mrow>\\n<mml:mrow>\\n<mml:mo>-</mml:mo>\\n<mml:msqrt>\\n<mml:mi>Ξ</mml:mi>\\n</mml:msqrt>\\n<mml:mrow>\\n<mml:mi>cos</mml:mi>\\n<mml:mfenced>\\n<mml:mi>Φ</mml:mi>\\n</mml:mfenced>\\n</mml:mrow>\\n</mml:mrow>\\n<mml:mo>+</mml:mo>\\n<mml:msup>\\n<mml:mtext>exp</mml:mtext>\\n<mml:msub>\\n<mml:mi>x</mml:mi>\\n<mml:mrow>\\n<mml:mi>D</mml:mi>\\n<mml:mi>Μ</mml:mi>\\n</mml:mrow>\\n</mml:msub>\\n</mml:msup>\\n</mml:mrow>\\n</mml:mfenced>\\n</mml:mrow>', '<mml:mi>J</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:mfrac>\\n<mml:mrow>\\n<mml:mo>∂</mml:mo>\\n</mml:mrow>\\n<mml:mrow>\\n<mml:mo>∂</mml:mo>\\n<mml:msub>\\n<mml:mi>U</mml:mi>\\n<mml:mrow>\\n<mml:mi>i</mml:mi>\\n<mml:mi>V</mml:mi>\\n<mml:mi>v</mml:mi>\\n</mml:mrow>\\n</mml:msub>\\n</mml:mrow>\\n</mml:mfrac>\\n<mml:mrow>\\n<mml:mfrac>\\n<mml:mrow>\\n<mml:mi>log</mml:mi>\\n<mml:mfenced>\\n<mml:msub>\\n<mml:mi>U</mml:mi>\\n<mml:mrow>\\n<mml:mi>i</mml:mi>\\n<mml:mi>V</mml:mi>\\n<mml:mi>v</mml:mi>\\n</mml:mrow>\\n</mml:msub>\\n</mml:mfenced>\\n</mml:mrow>\\n<mml:mi>τ</mml:mi>\\n</mml:mfrac>\\n</mml:mrow>\\n</mml:mrow>', '<mml:mi>a</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:msup>\\n<mml:mtext>exp</mml:mtext>\\n<mml:mi>Γ</mml:mi>\\n</mml:msup>\\n<mml:mo>-</mml:mo>\\n<mml:mrow>\\n<mml:mi>cos</mml:mi>\\n<mml:mfenced>\\n<mml:mi>K</mml:mi>\\n</mml:mfenced>\\n</mml:mrow>\\n</mml:mrow>', '<mml:msub>\\n<mml:mi>G</mml:mi>\\n<mml:mrow>\\n<mml:mi>9</mml:mi>\\n<mml:mi>O</mml:mi>\\n<mml:mi>W</mml:mi>\\n</mml:mrow>\\n</mml:msub>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:mrow>\\n<mml:mn>6</mml:mn>\\n<mml:mi>ι</mml:mi>\\n</mml:mrow>\\n<mml:mo>+</mml:mo>\\n<mml:mrow>\\n<mml:mi>tan</mml:mi>\\n<mml:mfenced>\\n<mml:msub>\\n<mml:mi>n</mml:mi>\\n<mml:mi>ξ</mml:mi>\\n</mml:msub>\\n</mml:mfenced>\\n</mml:mrow>\\n</mml:mrow>', '<mml:msub>\\n<mml:mi>b</mml:mi>\\n<mml:mrow>\\n<mml:mi>K</mml:mi>\\n<mml:mi>ε</mml:mi>\\n</mml:mrow>\\n</mml:msub>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:mfrac>\\n<mml:mrow>\\n<mml:mo>∂</mml:mo>\\n</mml:mrow>\\n<mml:mrow>\\n<mml:mo>∂</mml:mo>\\n<mml:mi>Ν</mml:mi>\\n</mml:mrow>\\n</mml:mfrac>\\n<mml:mrow>\\n<mml:mrow>\\n<mml:mi>log</mml:mi>\\n<mml:mfenced>\\n<mml:mi>μ</mml:mi>\\n</mml:mfenced>\\n</mml:mrow>\\n<mml:mo>-</mml:mo>\\n<mml:mrow>\\n<mml:mi>log</mml:mi>\\n<mml:mfenced>\\n<mml:mrow>\\n<mml:mfrac>\\n<mml:mi>Ν</mml:mi>\\n<mml:mi>l</mml:mi>\\n</mml:mfrac>\\n</mml:mrow>\\n</mml:mfenced>\\n</mml:mrow>\\n</mml:mrow>\\n</mml:mrow>', '<mml:mi>Θ</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:mo>∫</mml:mo>\\n<mml:mfenced>\\n<mml:mrow>\\n<mml:msup>\\n<mml:mi>Φ</mml:mi>\\n<mml:mrow>\\n<mml:mfrac>\\n<mml:mrow>\\n<mml:mi>sin</mml:mi>\\n<mml:mfenced>\\n<mml:mi>j</mml:mi>\\n</mml:mfenced>\\n</mml:mrow>\\n<mml:mn>2</mml:mn>\\n</mml:mfrac>\\n</mml:mrow>\\n</mml:msup>\\n<mml:mo>+</mml:mo>\\n<mml:msup>\\n<mml:mtext>exp</mml:mtext>\\n<mml:mi>Χ</mml:mi>\\n</mml:msup>\\n</mml:mrow>\\n</mml:mfenced>\\n<mml:mi>d</mml:mi>\\n<mml:mi>Χ</mml:mi>\\n</mml:mrow>']\n"
     ]
    }
   ],
   "source": [
    "td = TokenizerDataset(10, \"Data/tokenizer_dataset.json\")\n",
    "# td.create()\n",
    "data = td.json_to_dataset()\n",
    "# mapped_data = td.map_atomic_tokens(data)\n",
    "for entry in td.data_iterator(data, 10):\n",
    "    print(entry)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁\\nh', (0, 2)),\n",
       " ('▁=', (2, 4)),\n",
       " (\"▁Symbol('h')\\nh_g\", (4, 20)),\n",
       " ('▁=', (20, 22)),\n",
       " (\"▁Symbol('h_g')\\nh_c\", (22, 40)),\n",
       " ('▁=', (40, 42)),\n",
       " (\"▁Symbol('h_c')\\ne\", (42, 58)),\n",
       " ('▁=', (58, 60)),\n",
       " ('▁Eq(h,', (60, 66)),\n",
       " ('▁h_g', (66, 70)),\n",
       " ('▁+', (70, 72)),\n",
       " ('▁h_c)', (72, 77))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"')] # don't lowercase\n",
    ")\n",
    "tokenizer.normalizer.normalize_str(py)\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathMLTokenizer(Tokenizer):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "       \n",
    "    def normalise(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No constructor defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 49\u001b[0m\n\u001b[0;32m     45\u001b[0m                 splits\u001b[38;5;241m.\u001b[39mappend((text[last_end:], last_end \u001b[38;5;241m+\u001b[39m offset))\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m splits\n\u001b[1;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m MathMLPyTokenizer()\n\u001b[0;32m     51\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(mml)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: No constructor defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())                  \n",
    "\n",
    "class MathMLPyTokenizer(pre_tokenizers.PreTokenizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pre_tokenize(self, data):\n",
    "        \n",
    "        patterns = [\n",
    "            # HTML/XML tags\n",
    "            r'<[^>]+>',\n",
    "            # Python string literals\n",
    "            r'\"\"\"[^\"]*\"\"\"', r\"'''[^']*'''\", r'\"[^\"]*\"', r\"'[^']*'\",\n",
    "            # Python keywords and operators\n",
    "            r'\\bdef\\b', r'\\bclass\\b', r'\\bfor\\b', r'\\bwhile\\b', r'\\bif\\b', r'\\belif\\b', r'\\belse\\b',\n",
    "            r'\\breturn\\b', r'\\bimport\\b', r'\\bfrom\\b', r'\\bas\\b', r'\\bwith\\b', r'\\btry\\b', r'\\bexcept\\b',\n",
    "            # Common Python syntax elements\n",
    "            r'==', r'!=', r'<=', r'>=', r'\\+=', r'-=', r'\\*=', r'/=', \n",
    "            r'=>', r'->',  # Function type hints and lambdas\n",
    "            r'\\bSymbol\\b', r'\\bEq\\b', r'\\bexp\\b', r'\\bsin\\b', r'\\bcos\\b', r'\\btan\\b', r'\\bdiff\\b',\n",
    "            \n",
    "            # Indentation (important for Python)\n",
    "            r'^\\s+'\n",
    "        ]\n",
    "\n",
    "        combined_pattern = '|'.join(f'({p})' for p in patterns)\n",
    "        regex = re.compile(combined_pattern, re.MULTILINE)\n",
    "        \n",
    "        splits = []\n",
    "        for text, offset in data:\n",
    "            last_end = 0\n",
    "            for match in regex.finditer(text):\n",
    "                start, end = match.span()\n",
    "                \n",
    "                if start > last_end:\n",
    "                    # Add text before the special token\n",
    "                    splits.append((text[last_end:start], last_end + offset))\n",
    "                \n",
    "                # Add the special token as a whole\n",
    "                splits.append((text[start:end], start + offset))\n",
    "                last_end = end\n",
    "            \n",
    "            if last_end < len(text):\n",
    "                # Add remaining text\n",
    "                splits.append((text[last_end:], last_end + offset))\n",
    "                \n",
    "        return splits\n",
    "\n",
    "x = MathMLPyTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(mml)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n', (0, 1)), ('<mml:mi>', (1, 9)), ('h', (9, 10)), ('</mml:mi>', (10, 19)), ('\\n', (19, 20)), ('<mml:mo>', (20, 28)), ('=', (28, 29)), ('</mml:mo>', (29, 38)), ('\\n', (38, 39)), ('<mml:mrow>', (39, 49)), ('\\n', (49, 50)), ('<mml:msub>', (50, 60)), ('\\n', (60, 61)), ('<mml:mi>', (61, 69)), ('h', (69, 70)), ('</mml:mi>', (70, 79)), ('\\n', (79, 80)), ('<mml:mi>', (80, 88)), ('c', (88, 89)), ('</mml:mi>', (89, 98)), ('\\n', (98, 99)), ('</mml:msub>', (99, 110)), ('\\n', (110, 111)), ('<mml:mo>', (111, 119)), ('+', (119, 120)), ('</mml:mo>', (120, 129)), ('\\n', (129, 130)), ('<mml:msub>', (130, 140)), ('\\n', (140, 141)), ('<mml:mi>', (141, 149)), ('h', (149, 150)), ('</mml:mi>', (150, 159)), ('\\n', (159, 160)), ('<mml:mi>', (160, 168)), ('g', (168, 169)), ('</mml:mi>', (169, 178)), ('\\n', (178, 179)), ('</mml:msub>', (179, 190)), ('\\n', (190, 191)), ('</mml:mrow>', (191, 202)), ('\\n  ', (202, 205))]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Create a base tokenizer (BPE, WordPiece, or Unigram)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Define patterns we want to protect during tokenization\n",
    "text_patterns = [\n",
    "            # HTML/XML tags\n",
    "            r'<[^>]+>',\n",
    "            # Python string literals\n",
    "            r'\"\"\"[^\"]*\"\"\"', r\"'''[^']*'''\", r'\"[^\"]*\"', r\"'[^']*'\",\n",
    "            # Python keywords and operators\n",
    "            r'\\bdef\\b', r'\\bclass\\b', r'\\bfor\\b', r'\\bwhile\\b', r'\\bif\\b', r'\\belif\\b', r'\\belse\\b',\n",
    "            r'\\breturn\\b', r'\\bimport\\b', r'\\bfrom\\b', r'\\bas\\b', r'\\bwith\\b', r'\\btry\\b', r'\\bexcept\\b',\n",
    "            # Common Python syntax elements\n",
    "            r'==', r'!=', r'<=', r'>=', r'\\+=', r'-=', r'\\*=', r'/=', \n",
    "            r'=>', r'->',  # Function type hints and lambdas\n",
    "            r'\\bSymbol\\b', r'\\bEq\\b', r'\\bexp\\b', r'\\bsin\\b', r'\\bcos\\b', r'\\btan\\b', r'\\bdiff\\b', # MathML\n",
    "            # Indentation (important for Python)\n",
    "            r'^\\s+',\n",
    "            r'\\s+' # Whitespace\n",
    "        ]\n",
    "\n",
    "# Set up the pre-tokenizer using Split with pattern\n",
    "pattern = '|'.join(text_patterns)\n",
    "split_pre_tokenizer = Split(pattern=Regex(pattern), behavior=\"isolated\")\n",
    "# split_pre_tokenizer = WhitespaceSplit()\n",
    "tokenizer.pre_tokenizer = split_pre_tokenizer\n",
    "\n",
    "# Set up other components \n",
    "tokenizer.normalizer = Sequence([NFKC()])\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "test_text = \"\"\"\n",
    "def my_function(x, y): return x + y\n",
    "class MyClass:\n",
    "    def method(self): pass\n",
    "import numpy as np\n",
    "<math><msup><mi>x</mi><mn>2</mn></msup></math>\n",
    "\"\"\"\n",
    "\n",
    "check = tokenizer.pre_tokenizer.pre_tokenize_str(mml)\n",
    "print(check)\n",
    "output = tokenizer.encode(test_text)\n",
    "print(output.tokens)\n",
    "print(output.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['def my_function(x, y)', 'class MyClass', 'def method(self)', 'import numpy', '<math>', '<msup>', '<mi>', '</mi>', '<mn>', '</mn>', '</msup>', '</math>']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"def my_function(x, y): return x + y\n",
    "class MyClass:\n",
    "    def method(self): pass\n",
    "import numpy as np\n",
    "<math><msup><mi>x</mi><mn>2</mn></msup></math>\n",
    "\"\"\"\n",
    "\n",
    "# Your combined regex pattern\n",
    "tag_pattern = r'<[^>]+>'\n",
    "python_patterns = [\n",
    "    r'def\\s+\\w+\\([^)]*\\)', r'class\\s+\\w+', r'import\\s+\\w+', r'from\\s+\\w+\\s+import',\n",
    "    r'if\\s+.*:', r'for\\s+.*:', r'while\\s+.*:', r'try:', r'except\\s+.*:',\n",
    "]\n",
    "pattern = '|'.join([tag_pattern] + python_patterns)\n",
    "\n",
    "# Test regex separately\n",
    "regex = re.compile(pattern)\n",
    "matches = regex.findall(test_text)\n",
    "print(\"Matches:\", matches)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
