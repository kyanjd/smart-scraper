{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers import T5Tokenizer, T5TokenizerFast, PreTrainedTokenizerFast, AutoTokenizer\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer, Regex\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import Metaspace, Split, WhitespaceSplit\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "\n",
    "import re\n",
    "import src.paper_to_equation.Generation.Equation_BaseDataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(src.paper_to_equation.Generation.Equation_BaseDataset)\n",
    "from src.paper_to_equation.Generation.Equation_BaseDataset import BaseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the default tokenizer is unsuitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World</s>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "mml = \"\"\"\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mo>=</mml:mo>\n",
    "<mml:mrow>\n",
    "<mml:msub>\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mi>c</mml:mi>\n",
    "</mml:msub>\n",
    "<mml:mo>+</mml:mo>\n",
    "<mml:msub>\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mi>g</mml:mi>\n",
    "</mml:msub>\n",
    "</mml:mrow>\n",
    "  \"\"\"\n",
    "\n",
    "py = \"\"\"\n",
    "h = Symbol('h')\n",
    "h_g = Symbol('h_g')\n",
    "h_c = Symbol('h_c')\n",
    "e = Eq(h, h_g + h_c)\"\"\"\n",
    "\n",
    "test = \"Hello\\nWorld\"\n",
    "\n",
    "tokens = tokenizer.encode(test)\n",
    "print(tokenizer.decode(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerDataset(BaseDataset):\n",
    "    def __init__(self, num):\n",
    "        super().__init__(num)\n",
    "\n",
    "    def get_columns(self):\n",
    "        return [\"mathml\", \"python\"]\n",
    "\n",
    "    def map_atomic_tokens(self, dataset):\n",
    "\n",
    "        tag_map = {\"<mml:mo>\": \"<MO>\", \"</mml:mo>\": \"</MO>\",\n",
    "                   \"<mml:mi>\": \"<MI>\", \"</mml:mi>\": \"</MI>\",\n",
    "                   \"<mml:msub>\": \"<MSUB>\", \"</mml:msub>\": \"</MSUB>\",\n",
    "                   \"<mml:msup>\": \"<MSUP>\", \"</mml:msup>\": \"</MSUP>\",\n",
    "                   \"<mml:mrow>\": \"<MROW>\", \"</mml:mrow>\": \"</MROW>\", \n",
    "                   \"<mml:mfrac>\": \"<MFRAC>\", \"</mml:mfrac>\": \"</MFRAC>\"}\n",
    "        \n",
    "        for entry in dataset:\n",
    "            mathml = entry[\"mathml\"]\n",
    "            for tag, token in tag_map.items():\n",
    "                mathml = mathml.replace(tag, token)\n",
    "            entry[\"mathml\"] = mathml\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def get_tag_list(self):\n",
    "        return [\"<mml:mo>\", \"</mml:mo>\",\n",
    "                \"<mml:mi>\", \"</mml:mi>\",\n",
    "                \"<mml:msub>\", \"</mml:msub>\",\n",
    "                \"<mml:msup>\", \"</mml:msup>\",\n",
    "                \"<mml:mrow>\", \"</mml:mrow>\",\n",
    "                \"<mml:mfrac>\", \"</mml:mfrac>\",\n",
    "                \"<mml:mtext>\", \"</mml:mtext>\"]\n",
    "    \n",
    "    def extract_tags(self, data):\n",
    "        tags = set()\n",
    "        for entry in data:\n",
    "            mathml = entry[\"mathml\"]\n",
    "            found_tags = re.findall(r\"<\\s*[/]?[a-zA-Z0-9]+[^>]*>\", mathml)  # Extract full tags\n",
    "            tags.update(found_tags)  # Append to set\n",
    "\n",
    "        return list(tags)\n",
    "        \n",
    "    def data_iterator(self, batch_size):\n",
    "        columns = self.get_columns()\n",
    "        for i in range(0, len(self.dataset), batch_size):\n",
    "            yield [f\"{data[columns[0]]} {data[columns[1]]} \"for data in self.dataset[i:i+batch_size]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class method (unsuccessful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No constructor defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m\n\u001b[0;32m     45\u001b[0m                 splits\u001b[38;5;241m.\u001b[39mappend((text[last_end:], last_end \u001b[38;5;241m+\u001b[39m offset))\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m splits\n\u001b[1;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m MathMLPyTokenizer()\n\u001b[0;32m     51\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(mml)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: No constructor defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())                  \n",
    "\n",
    "class MathMLPyTokenizer(pre_tokenizers.PreTokenizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pre_tokenize(self, data):\n",
    "        \n",
    "        patterns = [\n",
    "            # HTML/XML tags\n",
    "            r'<[^>]+>',\n",
    "            # Python string literals\n",
    "            r'\"\"\"[^\"]*\"\"\"', r\"'''[^']*'''\", r'\"[^\"]*\"', r\"'[^']*'\",\n",
    "            # Python keywords and operators\n",
    "            r'\\bdef\\b', r'\\bclass\\b', r'\\bfor\\b', r'\\bwhile\\b', r'\\bif\\b', r'\\belif\\b', r'\\belse\\b',\n",
    "            r'\\breturn\\b', r'\\bimport\\b', r'\\bfrom\\b', r'\\bas\\b', r'\\bwith\\b', r'\\btry\\b', r'\\bexcept\\b',\n",
    "            # Common Python syntax elements\n",
    "            r'==', r'!=', r'<=', r'>=', r'\\+=', r'-=', r'\\*=', r'/=', \n",
    "            r'=>', r'->',  # Function type hints and lambdas\n",
    "            r'\\bSymbol\\b', r'\\bEq\\b', r'\\bexp\\b', r'\\bsin\\b', r'\\bcos\\b', r'\\btan\\b', r'\\bdiff\\b',\n",
    "            \n",
    "            # Indentation (important for Python)\n",
    "            r'^\\s+'\n",
    "        ]\n",
    "\n",
    "        combined_pattern = '|'.join(f'({p})' for p in patterns)\n",
    "        regex = re.compile(combined_pattern, re.MULTILINE)\n",
    "        \n",
    "        splits = []\n",
    "        for text, offset in data:\n",
    "            last_end = 0\n",
    "            for match in regex.finditer(text):\n",
    "                start, end = match.span()\n",
    "                \n",
    "                if start > last_end:\n",
    "                    # Add text before the special token\n",
    "                    splits.append((text[last_end:start], last_end + offset))\n",
    "                \n",
    "                # Add the special token as a whole\n",
    "                splits.append((text[start:end], start + offset))\n",
    "                last_end = end\n",
    "            \n",
    "            if last_end < len(text):\n",
    "                # Add remaining text\n",
    "                splits.append((text[last_end:], last_end + offset))\n",
    "                \n",
    "        return splits\n",
    "\n",
    "x = MathMLPyTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(mml)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset creation timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 100%|██████████| 10000/10000 [01:39<00:00, 100.33it/s]\n",
      "Generating dataset:  92%|█████████▏| 9223/10000 [01:28<00:07, 104.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1:  99.67798781394958\n",
      "Time 2:  90.23344993591309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start1 = time.time()\n",
    "    td1 = TokenizerDataset(10000)\n",
    "    td1.create_dataset()\n",
    "    end1 = time.time()\n",
    "\n",
    "    start2 = time.time()\n",
    "    td2 = TokenizerDataset(10000)\n",
    "    td2.create_dataset_mthread()\n",
    "    end2 = time.time()\n",
    "\n",
    "    print(\"Time 1: \", end1 - start1)\n",
    "    print(\"Time 2: \", end2 - start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 100%|██████████| 10000/10000 [03:39<00:00, 45.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "td = TokenizerDataset(10000)\n",
    "td.create(filepath=\"Tokenizer_Files/TokenizerDataset.csv\")\n",
    "print(len(td.dataset))\n",
    "data_iterator = td.data_iterator(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "td = TokenizerDataset(10000)\n",
    "td.load_csv(\"Tokenizer_Files/TokenizerDataset.csv\")\n",
    "data_iterator = td.data_iterator(1000)\n",
    "print(len(td.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\")) # BPE tokenizer\n",
    "tokenizer = Tokenizer(models.Unigram()) # Unigram tokenizer\n",
    "\n",
    "text_patterns = [ # Patterns to split on, ensuring that the tags are kept intact\n",
    "            r'<[^>]+>', # MathML tags\n",
    "            r'\"\"\"[^\"]*\"\"\"', r\"'''[^']*'''\", r'\"[^\"]*\"', r\"'[^']*'\", # Python string literals    \n",
    "            r'\\bdef\\b', r'\\bclass\\b', r'\\bfor\\b', r'\\bwhile\\b', r'\\bif\\b', r'\\belif\\b', r'\\belse\\b', # Python keywords and operators\n",
    "            r'\\breturn\\b', r'\\bimport\\b', r'\\bfrom\\b', r'\\bas\\b', r'\\bwith\\b', r'\\btry\\b', r'\\bexcept\\b',\n",
    "            r'==', r'!=', r'<=', r'>=', r'\\+=', r'-=', r'\\*=', r'/=', # Common Python syntax elements \n",
    "            r'=>', r'->', # Function type hints and lambdas\n",
    "            r'\\bSymbol\\b', r'\\bEq\\b', r'\\bexp\\b', r'\\bsin\\b', r'\\bcos\\b', r'\\btan\\b', r'\\bdiff\\b', # MathML \n",
    "            r'^\\s+', # Indentation (important for Python)\n",
    "            r'\\s+' # Whitespace\n",
    "        ]\n",
    "\n",
    "pattern = '|'.join(text_patterns) # Set up the pre-tokenizer using Split with pattern\n",
    "split_pre_tokenizer = Split(pattern=Regex(pattern), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = split_pre_tokenizer\n",
    "\n",
    "tokenizer.normalizer = Sequence([NFKC()]) # Normalises unicode characters like greek letters\n",
    "# tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "tag_list = td.extract_tags(td.dataset)\n",
    "sympy_tags = [\"Symbol\", \"Eq\", \"exp\", \"sin\", \"cos\", \"tan\", \"diff\", \"log\", \"Sum\", \"Derivative\", \"Integral\", \"\\n\", \"\\r\"]\n",
    "prefix = [\"translate\", \"MathML\", \"to\", \"Python\", \":\"]\n",
    "# special_tokens = [\"[PAD]\", \"[BOS]\", \"[EOS]\", \"[UNK]\"] + tag_list\n",
    "custom_tokens = tag_list + sympy_tags + prefix\n",
    "special_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
    "\n",
    "# trainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=special_tokens) # BPE Trainer\n",
    "# print(custom_tokens)\n",
    "tokenizer.add_tokens(custom_tokens)\n",
    "trainer = trainers.UnigramTrainer(vocab_size=30000, special_tokens=special_tokens) # Unigram Trainer\n",
    "tokenizer.train_from_iterator(iterator=data_iterator, trainer=trainer)\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\", # Single sentence template\n",
    "    pair=\"<s> $A </s> $B </s>\", # Pair template\n",
    "    special_tokens=[(\"<s>\", tokenizer.token_to_id(\"<s>\")), (\"</s>\", tokenizer.token_to_id(\"</s>\"))] # Special tokens for the templates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\kyanj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_login_key = os.environ.get(\"HF_LOGIN_KEY\")\n",
    "login(token=hf_login_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"Tokenizer_Files/mathml-py-tokenizer-unigram-v3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v3\\\\tokenizer_config.json',\n",
       " 'Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v3\\\\special_tokens_map.json',\n",
       " 'Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v3\\\\tokenizer.json')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast(\n",
    "    tokenizer_file=\"Tokenizer_Files/mathml-py-tokenizer-unigram-v3.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    pad_token=\"<pad>\",\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\"Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kj821/mathml-py-tokenizer-unigram-T5wrapped-v3/commit/891645f88fc52f76ec2a415e3c09c5bf821e49d3', commit_message='Upload tokenizer', commit_description='', oid='891645f88fc52f76ec2a415e3c09c5bf821e49d3', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"mathml-py-tokenizer-unigram-T5wrapped-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'T5TokenizerFast' object has no attribute 'pre_tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m test_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mdef my_function(x, y): return x + y\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mclass MyClass:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m<math><msup><mi>x</mi><mn>2</mn></msup></math>\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m check \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mpre_tokenize_str(td\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(check)\n\u001b[0;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(test_text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'T5TokenizerFast' object has no attribute 'pre_tokenizer'"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"\n",
    "def my_function(x, y): return x + y\n",
    "class MyClass:\n",
    "    def method(self): pass\n",
    "import numpy as np\n",
    "<math><msup><mi>x</mi><mn>2</mn></msup></math>\n",
    "\"\"\"\n",
    "\n",
    "check = tokenizer.pre_tokenizer.pre_tokenize_str(td.dataset[0][\"python\"])\n",
    "print(check)\n",
    "output = tokenizer.encode(test_text)\n",
    "print(tokenizer.decode(output.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  e l l o \n",
      " W o r l d\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v3\")\n",
    "\n",
    "data_files = {\"train\": \"Data/t5_train_2.csv\", \"validation\": \"Data/t5_validation_2.csv\", \"test\": \"Data/t5_test_2.csv\"}\n",
    "mml_py_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# output_mathml = tokenizer.encode(td.dataset[0][\"mathml\"])\n",
    "# output_py = tokenizer.encode(td.dataset[0][\"python\"])\n",
    "# print(output_py.tokens)\n",
    "# print(output_py.tokens)\n",
    "# print(output_py.ids)\n",
    "# print(tokenizer.decode(output_py.ids))\n",
    "# output_test = tokenizer.encode(td)\n",
    "# print(tokenizer.tokenize(td.dataset[0][\"mathml\"]))\n",
    "\n",
    "# print(tokenizer.tokenize(check))\n",
    "data = mml_py_dataset[\"train\"][0][\"MathML\"]\n",
    "# print(check)\n",
    "\n",
    "\n",
    "\n",
    "# print(repr(data))\n",
    "# print(repr(check))\n",
    "# print(tokenizer.encode(data).tokens)\n",
    "ids = tokenizer.encode(\"Hello\\r\\nWorld\")\n",
    "print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mml:msub><mml:mi>N</mml:mi>\n",
      "[5, 24, 46, 167, 27, 2]\n",
      "<mml:msub><mml:mi> N</mml:mi></s>\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v3\")\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"kj821/mathml-py-tokenizer-sentencepiece-v1\")\n",
    "\n",
    "data_files = {\"train\": \"Data/t5_train_2.csv\", \"validation\": \"Data/t5_validation_2.csv\", \"test\": \"Data/t5_test_2.csv\"}\n",
    "mml_py_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "data = mml_py_dataset[\"train\"][0][\"MathML\"]\n",
    "data = \"<mml:msub><mml:mi>N</mml:mi>\"\n",
    "# data = \"<mml:mi>h</mml:mi>\"\n",
    "print(data)\n",
    "\n",
    "# Tokenize it\n",
    "encodings = tokenizer.encode(data)\n",
    "print(encodings)\n",
    "\n",
    "# Print token IDs\n",
    "\n",
    "\n",
    "# Decode without skipping special tokens\n",
    "decoded = tokenizer.decode(encodings, skip_special_tokens=False)\n",
    "print(decoded)\n",
    "\n",
    "# # Decode with skipping special tokens (normal inference behavior)\n",
    "# decoded_skipped = tokenizer.decode(encodings, skip_special_tokens=True)\n",
    "# print(\"Decoded (skip special tokens):\", decoded_skipped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(\"Tokenizer_Files/TokenizerDataset.csv\")\n",
    "\n",
    "# Flatten MathML and Python\n",
    "def flatten(text):\n",
    "    return str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "mathml_texts = [flatten(item) for item in df[\"mathml\"]]\n",
    "python_texts = [flatten(item) for item in df[\"python\"]]\n",
    "\n",
    "# Write alternating MathML and Python lines\n",
    "with open(\"Tokenizer_Files/TokenizerCorpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for mathml, python in zip(mathml_texts, python_texts):\n",
    "        f.write(mathml + \"\\n\")\n",
    "        f.write(python + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = \"Tokenizer_Files/spm-mathml-py-tokenizer/tokenizer_v1\"\n",
    "vocab_size = 3800\n",
    "character_coverage = 1.0\n",
    "model_type = \"unigram\"\n",
    "\n",
    "td.load_csv(\"Tokenizer_Files/TokenizerDataset.csv\")\n",
    "tag_list = td.extract_tags(td.dataset)\n",
    "sympy_tags = [\"Symbol\", \"Eq\", \"exp\", \"sin\", \"cos\", \"tan\", \"diff\", \"log\", \"Sum\", \"Derivative\", \"Integral\", \"\\n\", \"\\r\"]\n",
    "prefix = [\"translate\", \"MathML\", \"to\", \"Python\", \":\"]\n",
    "user_defined_symbols = tag_list + sympy_tags + prefix\n",
    "special_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=\"Tokenizer_Files/TokenizerCorpus.txt\",\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    character_coverage=character_coverage,\n",
    "    model_type=model_type,\n",
    "    user_defined_symbols=user_defined_symbols,\n",
    "    pad_id=0,\n",
    "    bos_id=1,\n",
    "    eos_id=2,\n",
    "    unk_id=3,\n",
    "    control_symbols=[\"<s>\", \"</s>\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_config = {\n",
    "    \"model_max_length\": 512,\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "}\n",
    "\n",
    "special_tokens_map = {\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "}\n",
    "\n",
    "with open(\"Tokenizer_Files/spm-mathml-py-tokenizer/tokenizer_config.json\", \"w\") as f:\n",
    "    json.dump(tokenizer_config, f, indent=4)\n",
    "\n",
    "with open(\"Tokenizer_Files/spm-mathml-py-tokenizer/special_tokens_map.json\", \"w\") as f:\n",
    "    json.dump(special_tokens_map, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tokenizer_Files/spm-mathml-py-tokenizer\\\\tokenizer_config.json',\n",
       " 'Tokenizer_Files/spm-mathml-py-tokenizer\\\\special_tokens_map.json',\n",
       " 'Tokenizer_Files/spm-mathml-py-tokenizer\\\\spiece.model',\n",
       " 'Tokenizer_Files/spm-mathml-py-tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer(\n",
    "    vocab_file=\"Tokenizer_Files/spm-mathml-py-tokenizer/tokenizer_v1.model\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\"Tokenizer_Files/spm-mathml-py-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kj821/mathml-py-tokenizer-sentencepiece-v1/commit/60b6b568a89ebf47807706ea71b6a8c741a0f01d', commit_message='Upload tokenizer', commit_description='', oid='60b6b568a89ebf47807706ea71b6a8c741a0f01d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"mathml-py-tokenizer-sentencepiece-v1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
