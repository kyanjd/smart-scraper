{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "from transformers import T5Tokenizer, T5TokenizerFast, PreTrainedTokenizerFast, AutoTokenizer\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer, Regex\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import Metaspace, Split, WhitespaceSplit\n",
    "from huggingface_hub import login\n",
    "\n",
    "import re\n",
    "import src.paper_to_equation.Generation.Equation_BaseDataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(src.paper_to_equation.Generation.Equation_BaseDataset)\n",
    "from src.paper_to_equation.Generation.Equation_BaseDataset import BaseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the default tokenizer is unsuitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'h', '▁=', '▁', 'Symbol', '(', \"'\", 'h', \"'\", ')', '▁', 'h', '_', 'g', '▁=', '▁', 'Symbol', '(', \"'\", 'h', '_', 'g', \"'\", ')', '▁', 'h', '_', 'c', '▁=', '▁', 'Symbol', '(', \"'\", 'h', '_', 'c', \"'\", ')', '▁', 'e', '▁=', '▁E', 'q', '(', 'h', ',', '▁', 'h', '_', 'g', '▁+', '▁', 'h', '_', 'c', ')']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "mml = \"\"\"\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mo>=</mml:mo>\n",
    "<mml:mrow>\n",
    "<mml:msub>\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mi>c</mml:mi>\n",
    "</mml:msub>\n",
    "<mml:mo>+</mml:mo>\n",
    "<mml:msub>\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mi>g</mml:mi>\n",
    "</mml:msub>\n",
    "</mml:mrow>\n",
    "  \"\"\"\n",
    "\n",
    "py = \"\"\"\n",
    "h = Symbol('h')\n",
    "h_g = Symbol('h_g')\n",
    "h_c = Symbol('h_c')\n",
    "e = Eq(h, h_g + h_c)\"\"\"\n",
    "\n",
    "tokens = tokenizer.tokenize(py)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerDataset(BaseDataset):\n",
    "    def __init__(self, num):\n",
    "        super().__init__(num)\n",
    "\n",
    "    def get_columns(self):\n",
    "        return [\"mathml\", \"python\"]\n",
    "\n",
    "    def map_atomic_tokens(self, dataset):\n",
    "\n",
    "        tag_map = {\"<mml:mo>\": \"<MO>\", \"</mml:mo>\": \"</MO>\",\n",
    "                   \"<mml:mi>\": \"<MI>\", \"</mml:mi>\": \"</MI>\",\n",
    "                   \"<mml:msub>\": \"<MSUB>\", \"</mml:msub>\": \"</MSUB>\",\n",
    "                   \"<mml:msup>\": \"<MSUP>\", \"</mml:msup>\": \"</MSUP>\",\n",
    "                   \"<mml:mrow>\": \"<MROW>\", \"</mml:mrow>\": \"</MROW>\", \n",
    "                   \"<mml:mfrac>\": \"<MFRAC>\", \"</mml:mfrac>\": \"</MFRAC>\"}\n",
    "        \n",
    "        for entry in dataset:\n",
    "            mathml = entry[\"mathml\"]\n",
    "            for tag, token in tag_map.items():\n",
    "                mathml = mathml.replace(tag, token)\n",
    "            entry[\"mathml\"] = mathml\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def get_tag_list(self):\n",
    "        return [\"<mml:mo>\", \"</mml:mo>\",\n",
    "                \"<mml:mi>\", \"</mml:mi>\",\n",
    "                \"<mml:msub>\", \"</mml:msub>\",\n",
    "                \"<mml:msup>\", \"</mml:msup>\",\n",
    "                \"<mml:mrow>\", \"</mml:mrow>\",\n",
    "                \"<mml:mfrac>\", \"</mml:mfrac>\",\n",
    "                \"<mml:mtext>\", \"</mml:mtext>\"]\n",
    "    \n",
    "    def extract_tags(self, data):\n",
    "        tags = set()\n",
    "        for entry in data:\n",
    "            mathml = entry[\"mathml\"]\n",
    "            found_tags = re.findall(r\"<\\s*[/]?[a-zA-Z0-9]+[^>]*>\", mathml)  # Extract full tags\n",
    "            tags.update(found_tags)  # Append to set\n",
    "\n",
    "        return list(tags)\n",
    "        \n",
    "    def data_iterator(self, batch_size):\n",
    "        columns = self.get_columns()\n",
    "        for i in range(0, len(self.dataset), batch_size):\n",
    "            yield [f\"{data[columns[0]]} {data[columns[1]]} \"for data in self.dataset[i:i+batch_size]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class method (unsuccessful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No constructor defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m\n\u001b[0;32m     45\u001b[0m                 splits\u001b[38;5;241m.\u001b[39mappend((text[last_end:], last_end \u001b[38;5;241m+\u001b[39m offset))\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m splits\n\u001b[1;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m MathMLPyTokenizer()\n\u001b[0;32m     51\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(mml)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: No constructor defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())                  \n",
    "\n",
    "class MathMLPyTokenizer(pre_tokenizers.PreTokenizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pre_tokenize(self, data):\n",
    "        \n",
    "        patterns = [\n",
    "            # HTML/XML tags\n",
    "            r'<[^>]+>',\n",
    "            # Python string literals\n",
    "            r'\"\"\"[^\"]*\"\"\"', r\"'''[^']*'''\", r'\"[^\"]*\"', r\"'[^']*'\",\n",
    "            # Python keywords and operators\n",
    "            r'\\bdef\\b', r'\\bclass\\b', r'\\bfor\\b', r'\\bwhile\\b', r'\\bif\\b', r'\\belif\\b', r'\\belse\\b',\n",
    "            r'\\breturn\\b', r'\\bimport\\b', r'\\bfrom\\b', r'\\bas\\b', r'\\bwith\\b', r'\\btry\\b', r'\\bexcept\\b',\n",
    "            # Common Python syntax elements\n",
    "            r'==', r'!=', r'<=', r'>=', r'\\+=', r'-=', r'\\*=', r'/=', \n",
    "            r'=>', r'->',  # Function type hints and lambdas\n",
    "            r'\\bSymbol\\b', r'\\bEq\\b', r'\\bexp\\b', r'\\bsin\\b', r'\\bcos\\b', r'\\btan\\b', r'\\bdiff\\b',\n",
    "            \n",
    "            # Indentation (important for Python)\n",
    "            r'^\\s+'\n",
    "        ]\n",
    "\n",
    "        combined_pattern = '|'.join(f'({p})' for p in patterns)\n",
    "        regex = re.compile(combined_pattern, re.MULTILINE)\n",
    "        \n",
    "        splits = []\n",
    "        for text, offset in data:\n",
    "            last_end = 0\n",
    "            for match in regex.finditer(text):\n",
    "                start, end = match.span()\n",
    "                \n",
    "                if start > last_end:\n",
    "                    # Add text before the special token\n",
    "                    splits.append((text[last_end:start], last_end + offset))\n",
    "                \n",
    "                # Add the special token as a whole\n",
    "                splits.append((text[start:end], start + offset))\n",
    "                last_end = end\n",
    "            \n",
    "            if last_end < len(text):\n",
    "                # Add remaining text\n",
    "                splits.append((text[last_end:], last_end + offset))\n",
    "                \n",
    "        return splits\n",
    "\n",
    "x = MathMLPyTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(mml)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset creation timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 100%|██████████| 10000/10000 [01:39<00:00, 100.33it/s]\n",
      "Generating dataset:  92%|█████████▏| 9223/10000 [01:28<00:07, 104.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1:  99.67798781394958\n",
      "Time 2:  90.23344993591309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start1 = time.time()\n",
    "    td1 = TokenizerDataset(10000)\n",
    "    td1.create_dataset()\n",
    "    end1 = time.time()\n",
    "\n",
    "    start2 = time.time()\n",
    "    td2 = TokenizerDataset(10000)\n",
    "    td2.create_dataset_mthread()\n",
    "    end2 = time.time()\n",
    "\n",
    "    print(\"Time 1: \", end1 - start1)\n",
    "    print(\"Time 2: \", end2 - start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 100%|██████████| 10000/10000 [03:48<00:00, 43.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "td = TokenizerDataset(10000)\n",
    "td.create_dataset()\n",
    "print(len(td.dataset))\n",
    "data_iterator = td.data_iterator(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<mml:mo>', '</mml:mo>', '<mml:mi>', '</mml:mi>', '<mml:msub>', '</mml:msub>', '<mml:msup>', '</mml:msup>', '<mml:mrow>', '</mml:mrow>', '<mml:mfrac>', '</mml:mfrac>', '<mml:mtext>', '</mml:mtext>']\n",
      "['</mml:msup>', '<mml:msub>', '<mml:mtext>', '<mml:mrow>', '</mml:msqrt>', '</mml:mrow>', '<mml:mfrac>', '</mml:mn>', '<mml:msqrt>', '<mml:mfenced>', '</mml:msubsup>', '</mml:mfrac>', '<mml:msubsup>', '</mml:mo>', '<mml:mi>', '<mml:mo>', '<mml:mn>', '</mml:msub>', '<mml:msup>', '</mml:munderover>', '</mml:mi>', '</mml:mfenced>', '</mml:mtext>', '<mml:munderover>']\n"
     ]
    }
   ],
   "source": [
    "print(td.get_tag_list())\n",
    "print(td.extract_tags(td.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\")) # BPE tokenizer\n",
    "tokenizer = Tokenizer(models.Unigram()) # Unigram tokenizer\n",
    "\n",
    "text_patterns = [ # Patterns to split on, ensuring that the tags are kept intact\n",
    "            r'<[^>]+>', # MathML tags\n",
    "            r'\"\"\"[^\"]*\"\"\"', r\"'''[^']*'''\", r'\"[^\"]*\"', r\"'[^']*'\", # Python string literals    \n",
    "            r'\\bdef\\b', r'\\bclass\\b', r'\\bfor\\b', r'\\bwhile\\b', r'\\bif\\b', r'\\belif\\b', r'\\belse\\b', # Python keywords and operators\n",
    "            r'\\breturn\\b', r'\\bimport\\b', r'\\bfrom\\b', r'\\bas\\b', r'\\bwith\\b', r'\\btry\\b', r'\\bexcept\\b',\n",
    "            r'==', r'!=', r'<=', r'>=', r'\\+=', r'-=', r'\\*=', r'/=', # Common Python syntax elements \n",
    "            r'=>', r'->', # Function type hints and lambdas\n",
    "            r'\\bSymbol\\b', r'\\bEq\\b', r'\\bexp\\b', r'\\bsin\\b', r'\\bcos\\b', r'\\btan\\b', r'\\bdiff\\b', # MathML \n",
    "            r'^\\s+', # Indentation (important for Python)\n",
    "            r'\\s+' # Whitespace\n",
    "        ]\n",
    "\n",
    "pattern = '|'.join(text_patterns) # Set up the pre-tokenizer using Split with pattern\n",
    "split_pre_tokenizer = Split(pattern=Regex(pattern), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = split_pre_tokenizer\n",
    "\n",
    "tokenizer.normalizer = Sequence([NFKC()]) # Normalises unicode characters like greek letters\n",
    "# tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "tag_list = td.extract_tags(td.dataset)\n",
    "sympy_tags = [\"Symbol\", \"Eq\", \"exp\", \"sin\", \"cos\", \"tan\", \"diff\", \"log\", \"Sum\", \"Derivative\", \"Integral\"]\n",
    "prefix = [\"translate\", \"MathML\", \"to\", \"Python\", \":\"]\n",
    "# special_tokens = [\"[PAD]\", \"[BOS]\", \"[EOS]\", \"[UNK]\"] + tag_list\n",
    "special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"] + tag_list + sympy_tags + prefix\n",
    "\n",
    "# trainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=special_tokens) # BPE Trainer\n",
    "trainer = trainers.UnigramTrainer(vocab_size=30000, special_tokens=special_tokens) # Unigram Trainer\n",
    "tokenizer.train_from_iterator(iterator=data_iterator, trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\kyanj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_login_key = os.environ.get(\"HF_LOGIN_KEY\")\n",
    "login(token=hf_login_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"Tokenizer_Files/mathml-py-tokenizer-unigram-v2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v2\\\\tokenizer_config.json',\n",
       " 'Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v2\\\\special_tokens_map.json',\n",
       " 'Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast(\n",
    "    tokenizer_file=\"Tokenizer_Files/mathml-py-tokenizer-unigram-v2.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    additional_special_tokens=tag_list\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\"Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kj821/mathml-py-tokenizer-unigram-T5wrapped-v2/commit/7406d655b27d7755fe88e3e839ec13d96aca34c4', commit_message='Upload tokenizer', commit_description='', oid='7406d655b27d7755fe88e3e839ec13d96aca34c4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"mathml-py-tokenizer-unigram-T5wrapped-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "def my_function(x, y): return x + y\n",
    "class MyClass:\n",
    "    def method(self): pass\n",
    "import numpy as np\n",
    "<math><msup><mi>x</mi><mn>2</mn></msup></math>\n",
    "\"\"\"\n",
    "\n",
    "check = tokenizer.pre_tokenizer.pre_tokenize_str(mml)\n",
    "print(check)\n",
    "output = tokenizer.encode(test_text)\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<mml:msub>', '\\n', '<mml:mi>', 'N', '</mml:mi>', '\\n', '<mml:mi>', 'P', '</mml:mi>', '\\n', '</mml:msub>', '\\n', '<mml:mo>', '=', '</mml:mo>', '\\n', '<mml:mrow>', '\\n', '<mml:munderover>', '\\n', '<mml:mo>', '∑', '</mml:mo>', '\\n', '<mml:mrow>', '\\n', '<mml:mi>', 't', '</mml:mi>', '\\n', '<mml:mo>', '=', '</mml:mo>', '\\n', '<mml:mn>', '7', '</mml:mn>', '\\n', '</mml:mrow>', '\\n', '<mml:msub>', '\\n', '<mml:mi>', 'Ο', '</mml:mi>', '\\n', '<mml:mrow>', '\\n', '<mml:mi>', 'O', '</mml:mi>', '\\n', '<mml:mi>', 'ι', '</mml:mi>', '\\n', '<mml:mi>', 'χ', '</mml:mi>', '\\n', '</mml:mrow>', '\\n', '</mml:msub>', '\\n', '</mml:munderover>', '\\n', '<mml:mfenced>', '\\n', '<mml:mrow>', '\\n', '<mml:msup>', '\\n', '<mml:msup>', '\\n', '<mml:mtext>', 'exp', '</mml:mtext>', '\\n', '<mml:mi>', 'φ', '</mml:mi>', '\\n', '</mml:msup>', '\\n', '<mml:msub>', '\\n', '<mml:mi>', 'Μ', '</mml:mi>', '\\n', '<mml:mrow>', '\\n', '<mml:mi>', 'ω', '</mml:mi>', '\\n', '<mml:mi>', 'L', '</mml:mi>', '\\n', '</mml:mrow>', '\\n', '</mml:msub>', '\\n', '</mml:msup>', '\\n', '<mml:mo>', '+', '</mml:mo>', '\\n', '<mml:mrow>', '\\n', '<mml:mi>', 'tan', '</mml:mi>', '\\n', '<mml:mfenced>', '\\n', '<mml:msub>', '\\n', '<mml:mi>', 'η', '</mml:mi>', '\\n', '<mml:mrow>', '\\n', '<mml:mi>', 'y', '</mml:mi>', '\\n', '<mml:mi>', 'λ', '</mml:mi>', '\\n', '</mml:mrow>', '\\n', '</mml:msub>', '\\n', '</mml:mfenced>', '\\n', '</mml:mrow>', '\\n', '</mml:mrow>', '\\n', '</mml:mfenced>', '\\n', '</mml:mrow>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Tokenizer_Files/mathml-py-tokenizer-unigram-T5wrapped-v2\")\n",
    "\n",
    "# output_mathml = tokenizer.encode(td.dataset[0][\"mathml\"])\n",
    "# output_py = tokenizer.encode(td.dataset[0][\"python\"])\n",
    "# print(output_py.tokens)\n",
    "# print(output_py.tokens)\n",
    "# print(output_py.ids)\n",
    "# print(tokenizer.decode(output_py.ids))\n",
    "# output_test = tokenizer.encode(td)\n",
    "# print(tokenizer.tokenize(td.dataset[0][\"mathml\"]))\n",
    "check = \"\"\"<mml:msub>\n",
    "<mml:mi>N</mml:mi>\n",
    "<mml:mi>P</mml:mi>\n",
    "</mml:msub>\n",
    "<mml:mo>=</mml:mo>\n",
    "<mml:mrow>\n",
    "<mml:munderover>\n",
    "<mml:mo>∑</mml:mo>\n",
    "<mml:mrow>\n",
    "<mml:mi>t</mml:mi>\n",
    "<mml:mo>=</mml:mo>\n",
    "<mml:mn>7</mml:mn>\n",
    "</mml:mrow>\n",
    "<mml:msub>\n",
    "<mml:mi>Ο</mml:mi>\n",
    "<mml:mrow>\n",
    "<mml:mi>O</mml:mi>\n",
    "<mml:mi>ι</mml:mi>\n",
    "<mml:mi>χ</mml:mi>\n",
    "</mml:mrow>\n",
    "</mml:msub>\n",
    "</mml:munderover>\n",
    "<mml:mfenced>\n",
    "<mml:mrow>\n",
    "<mml:msup>\n",
    "<mml:msup>\n",
    "<mml:mtext>exp</mml:mtext>\n",
    "<mml:mi>φ</mml:mi>\n",
    "</mml:msup>\n",
    "<mml:msub>\n",
    "<mml:mi>Μ</mml:mi>\n",
    "<mml:mrow>\n",
    "<mml:mi>ω</mml:mi>\n",
    "<mml:mi>L</mml:mi>\n",
    "</mml:mrow>\n",
    "</mml:msub>\n",
    "</mml:msup>\n",
    "<mml:mo>+</mml:mo>\n",
    "<mml:mrow>\n",
    "<mml:mi>tan</mml:mi>\n",
    "<mml:mfenced>\n",
    "<mml:msub>\n",
    "<mml:mi>η</mml:mi>\n",
    "<mml:mrow>\n",
    "<mml:mi>y</mml:mi>\n",
    "<mml:mi>λ</mml:mi>\n",
    "</mml:mrow>\n",
    "</mml:msub>\n",
    "</mml:mfenced>\n",
    "</mml:mrow>\n",
    "</mml:mrow>\n",
    "</mml:mfenced>\n",
    "</mml:mrow>\"\"\"\n",
    "\n",
    "print(tokenizer.tokenize(check))\n",
    "# print(tokenizer.tokenize(mml_py_dataset[\"train\"][0][\"MathML\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
