{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from src.paper_to_equation.Generation.Equation_BaseDataset import BaseDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Dataset(BaseDataset):\n",
    "    def __init__(self, num, filepath):\n",
    "        super().__init__(num, filepath)\n",
    "\n",
    "    def get_columns(self):\n",
    "        return [\"MathML\", \"Python\"]\n",
    "\n",
    "t5_data = T5Dataset(100, \"t5_validation_1.csv\")\n",
    "t5_data.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "XMLSyntaxError",
     "evalue": "Opening and ending tag mismatch: mi line 11 and m, line 11, column 18 (<string>, line 11)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[16], line 57\u001b[0m\n    sympy_expr = parse_mathml(mathml_expr)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[16], line 6\u001b[0m in \u001b[0;35mparse_mathml\u001b[0m\n    root = etree.fromstring(mathml)  # Parse XML\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32msrc\\\\lxml\\\\etree.pyx:3306\u001b[0m in \u001b[0;35mlxml.etree.fromstring\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32msrc\\\\lxml\\\\parser.pxi:1995\u001b[0m in \u001b[0;35mlxml.etree._parseMemoryDocument\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32msrc\\\\lxml\\\\parser.pxi:1875\u001b[0m in \u001b[0;35mlxml.etree._parseDoc\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32msrc\\\\lxml\\\\parser.pxi:1105\u001b[0m in \u001b[0;35mlxml.etree._BaseParser._parseUnicodeDoc\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32msrc\\\\lxml\\\\parser.pxi:633\u001b[0m in \u001b[0;35mlxml.etree._ParserContext._handleParseResultDoc\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32msrc\\\\lxml\\\\parser.pxi:743\u001b[0m in \u001b[0;35mlxml.etree._handleParseResult\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32msrc\\\\lxml\\\\parser.pxi:672\u001b[1;36m in \u001b[1;35mlxml.etree._raiseParseError\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>:11\u001b[1;36m\u001b[0m\n\u001b[1;31mXMLSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Opening and ending tag mismatch: mi line 11 and m, line 11, column 18\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, Eq, Add, Mul, Rational\n",
    "from lxml import etree\n",
    "\n",
    "# Recursive function to parse MathML into SymPy\n",
    "def parse_mathml(mathml):\n",
    "    root = etree.fromstring(mathml)  # Parse XML\n",
    "    return convert_to_sympy(root)\n",
    "\n",
    "def convert_to_sympy(element):\n",
    "    tag = element.tag.split(\"}\")[-1]  # Remove namespace (e.g., {MathML}mrow → mrow)\n",
    "\n",
    "    if tag == \"mi\":  # Variable (e.g., h, g, c)\n",
    "        return symbols(element.text)\n",
    "\n",
    "    elif tag == \"mo\":  # Operator (+, -, *, /, =)\n",
    "        return element.text.strip()\n",
    "\n",
    "    elif tag == \"msub\":  # Subscripted variables (e.g., h_g, h_c)\n",
    "        base, subscript = element.getchildren()\n",
    "        return symbols(f\"{convert_to_sympy(base)}_{convert_to_sympy(subscript)}\")\n",
    "\n",
    "    elif tag == \"mrow\":  # Math expressions inside <mrow>\n",
    "        children = element.getchildren()\n",
    "        expr = convert_to_sympy(children[0])\n",
    "        for i in range(1, len(children), 2):  # Operators appear at odd indices\n",
    "            op = convert_to_sympy(children[i])\n",
    "            right = convert_to_sympy(children[i + 1])\n",
    "            expr = {\n",
    "                \"=\": Eq,  # ✅ Correctly handles equations\n",
    "                \"+\": Add,\n",
    "                \"-\": lambda a, b: a - b,\n",
    "                \"*\": Mul,\n",
    "                \"/\": lambda a, b: Rational(a, b)\n",
    "            }.get(op, Add)(expr, right)\n",
    "        return expr\n",
    "\n",
    "    return None  # Fallback for unsupported cases\n",
    "\n",
    "# Example MathML input\n",
    "mathml_expr = '''\n",
    "<mrow xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "    <mi>h</mi>\n",
    "    <mo>=</mo>\n",
    "    <msub>\n",
    "        <mi>h</mi>\n",
    "        <mi>g</mi>\n",
    "    </msub>\n",
    "    <mo>+</mo>\n",
    "    <msub>\n",
    "        <mi>h</m>\n",
    "        <mi>c</mi>\n",
    "    </msub>\n",
    "</mrow>\n",
    "'''\n",
    "\n",
    "# Convert to SymPy expression\n",
    "sympy_expr = parse_mathml(mathml_expr)\n",
    "print(sympy_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=\n",
      "+\n",
      "h + h_c + h_g\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, Rational, Pow, Mul, Add\n",
    "from lxml import etree\n",
    "\n",
    "# Define a recursive function to parse MathML into SymPy\n",
    "def parse_mathml(mathml):\n",
    "    root = etree.fromstring(mathml)\n",
    "    return convert_to_sympy(root)\n",
    "\n",
    "def convert_to_sympy(element):\n",
    "    tag = element.tag.split(\"}\")[-1]  # Remove namespace (e.g., {http://www.w3.org/1998/Math/MathML}mrow → mrow)\n",
    "\n",
    "    if tag == \"mi\":  # Variable (x, y, h, etc.)\n",
    "        return symbols(element.text)\n",
    "\n",
    "    elif tag == \"mn\":  # Number (1, 2, etc.)\n",
    "        return Rational(element.text)\n",
    "\n",
    "    elif tag == \"mo\":  # Operator (+, -, *)\n",
    "        print(element.text.strip())\n",
    "        return element.text.strip()\n",
    "\n",
    "    elif tag == \"mfrac\":  # Fraction (a/b)\n",
    "        num, den = element.getchildren()\n",
    "        return Rational(convert_to_sympy(num), convert_to_sympy(den))\n",
    "\n",
    "    elif tag == \"msup\":  # Exponentiation (x^2)\n",
    "        base, exponent = element.getchildren()\n",
    "        return Pow(convert_to_sympy(base), convert_to_sympy(exponent))\n",
    "\n",
    "    elif tag == \"msub\":  # Subscripted variable (k_f, h_t, etc.)\n",
    "        base, subscript = element.getchildren()\n",
    "        return symbols(f\"{convert_to_sympy(base)}_{convert_to_sympy(subscript)}\")\n",
    "\n",
    "    elif tag == \"mrow\":  # Grouping of expressions (a + b)\n",
    "        children = element.getchildren()\n",
    "        expr = convert_to_sympy(children[0])\n",
    "        for i in range(1, len(children), 2):  # Every other element is an operator\n",
    "            op = convert_to_sympy(children[i])\n",
    "            right = convert_to_sympy(children[i + 1])\n",
    "            expr = {\"+\" : Add, \"-\" : Sub, \"*\" : Mul, \"/\" : Rational}.get(op, Add)(expr, right)\n",
    "        return expr\n",
    "\n",
    "    return None  # Fallback for unsupported cases\n",
    "\n",
    "# Example MathML Input (Super Complex Expression)\n",
    "mathml_expr = '''\n",
    "<mml:mrow xmlns:mml=\"http://www.w3.org/1998/Math/MathML\">\n",
    "    <mml:mi>h</mml:mi>\n",
    "    <mml:mo>=</mml:mo>\n",
    "    <mml:mfrac>\n",
    "        <mml:mrow>\n",
    "            <mml:mn>1</mml:mn>\n",
    "            <mml:mo>-</mml:mo>\n",
    "            <mml:mi>A</mml:mi>\n",
    "        </mml:mrow>\n",
    "        <mml:mrow>\n",
    "            <mml:msub>\n",
    "                <mml:mi>h</mml:mi>\n",
    "                <mml:mi>f</mml:mi>\n",
    "            </mml:msub>\n",
    "        </mml:mrow>\n",
    "    </mml:mfrac>\n",
    "    <mml:mfrac>\n",
    "        <mml:mrow>\n",
    "            <mml:mn>2</mml:mn>\n",
    "            <mml:msub>\n",
    "                <mml:mi>k</mml:mi>\n",
    "                <mml:mi>f</mml:mi>\n",
    "            </mml:msub>\n",
    "            <mml:msub>\n",
    "                <mml:mi>k</mml:mi>\n",
    "                <mml:mi>t</mml:mi>\n",
    "            </mml:msub>\n",
    "            <mml:msub>\n",
    "                <mml:mi>k</mml:mi>\n",
    "                <mml:mi>w</mml:mi>\n",
    "            </mml:msub>\n",
    "        </mml:mrow>\n",
    "        <mml:mrow>\n",
    "            <mml:mn>2</mml:mn>\n",
    "            <mml:msub>\n",
    "                <mml:mi>k</mml:mi>\n",
    "                <mml:mi>t</mml:mi>\n",
    "            </mml:msub>\n",
    "            <mml:msub>\n",
    "                <mml:mi>k</mml:mi>\n",
    "                <mml:mi>w</mml:mi>\n",
    "            </mml:msub>\n",
    "            <mml:mo>-</mml:mo>\n",
    "            <mml:msub>\n",
    "                <mml:mi>k</mml:mi>\n",
    "                <mml:mi>w</mml:mi>\n",
    "            </mml:msub>\n",
    "            <mml:msub>\n",
    "                <mml:mi>k</mml:mi>\n",
    "                <mml:mi>f</mml:mi>\n",
    "            </mml:msub>\n",
    "            <mml:mo>-</mml:mo>\n",
    "            <mml:msub>\n",
    "                <mml:mi>k</mml:mi>\n",
    "                <mml:mi>f</mml:mi>\n",
    "            </mml:msub>\n",
    "            <mml:msub>\n",
    "                <mml:mi>k</mml:mi>\n",
    "                <mml:mi>t</mml:mi>\n",
    "            </mml:msub>\n",
    "        </mml:mrow>\n",
    "    </mml:mfrac>\n",
    "</mml:mrow>\n",
    "'''\n",
    "mathml_expr = \"\"\"<mrow xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "    <mi>h</mi>\n",
    "    <mo>=</mo>\n",
    "    <msub>\n",
    "        <mi>h</mi>\n",
    "        <mi>g</mi>\n",
    "    </msub>\n",
    "    <mo>+</mo>\n",
    "    <msub>\n",
    "        <mi>h</mi>\n",
    "        <mi>c</mi>\n",
    "    </msub>\n",
    "</mrow>\"\"\"\n",
    "\n",
    "# Convert MathML to SymPy\n",
    "sympy_expr = parse_mathml(mathml_expr)\n",
    "print(sympy_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 500 examples [00:00, 9876.71 examples/s]\n",
      "Generating validation split: 100 examples [00:00, 21782.93 examples/s]\n",
      "Generating test split: 100 examples [00:00, 16131.94 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['MathML', 'Python'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['MathML', 'Python'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['MathML', 'Python'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MathML': '\\r\\n<mml:msup>\\r\\n<mml:msup>\\r\\n<mml:mtext>exp</mml:mtext>\\r\\n<mml:mi>r</mml:mi>\\r\\n</mml:msup>\\r\\n<mml:msup>\\r\\n<mml:mi>K</mml:mi>\\r\\n<mml:msup>\\r\\n<mml:mtext>exp</mml:mtext>\\r\\n<mml:mrow>\\r\\n<mml:mi>ι</mml:mi>\\r\\n<mml:mi>ρ</mml:mi>\\r\\n</mml:mrow>\\r\\n</mml:msup>\\r\\n</mml:msup>\\r\\n</mml:msup>\\r\\n<mml:mo>=</mml:mo>\\r\\n<mml:mrow>\\r\\n<mml:mi>cos</mml:mi>\\r\\n<mml:mfenced>\\r\\n<mml:mi>d</mml:mi>\\r\\n</mml:mfenced>\\r\\n</mml:mrow>\\r\\n',\n",
       " 'Python': \"ι = Symbol('ι')\\r\\nρ = Symbol('ρ')\\r\\nK = Symbol('K')\\r\\nr = Symbol('r')\\r\\nd = Symbol('d')\\r\\ne = Eq(exp(r)**(K**exp(ι*ρ)), cos(d))\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\"train\": \"t5_train_1.csv\", \"validation\": \"t5_validation_1.csv\", \"test\": \"t5_test_1.csv\"}\n",
    "mml_py_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "print(mml_py_dataset)\n",
    "display(mml_py_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2408.91 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1572.64 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1136.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "\n",
    "max_length = 1024\n",
    "def preprocess_function(examples):\n",
    "    prefix = \"translate: MathML to Python: \"\n",
    "    inputs = [prefix + mml for mml in examples[\"MathML\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"Python\"], max_length=max_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = mml_py_dataset.map(preprocess_function, batched=True, remove_columns=[\"MathML\", \"Python\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    3,   172,  3274,     3, 18650,   599,    31,   172,    31,    61,\n",
      "             3,    17,  3274,     3, 18650,   599,    31,    17,    31,    61,\n",
      "             3,     4,  3274,     3, 18650,   599,    31,     4,    31,    61,\n",
      "           454,  3274,     3, 18650,   599,    31,   566,    31,    61,     3,\n",
      "             2,  3274,     3, 18650,   599,    31,     2,    31,    61,     3,\n",
      "            15,  3274,   262,  1824,   599,  2152,   599,   172,    61,  1768,\n",
      "          4303,   599,    17,    61,    87,     4,     6,     3,    17,   152,\n",
      "           599,     2,    61, 19844,   994,   102,   599,   566,    61,    61,\n",
      "             1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  276,  3274,     3, 18650,   599,    31,   345,    31,    61,     3,\n",
      "             2,   834,     2,  3274,     3, 18650,   599,    31,     2,   834,\n",
      "             2,    31,    61,   445,   834,  1824,  3274,     3, 18650,   599,\n",
      "            31,   567,   834,  1824,    31,    61,   332,   834,     2,  3274,\n",
      "             3, 18650,   599,    31,   382,   834,     2,    31,    61,     3,\n",
      "             2,   834,     2,  3274,     3, 18650,   599,    31,     2,   834,\n",
      "             2,    31,    61,     3,   157,  3274,     3, 18650,   599,    31,\n",
      "           157,    31,    61,     3,     2,  3274,     3, 18650,   599,    31,\n",
      "             2,    31,    61,     3,    15,  3274,   262,  1824,   599,     7,\n",
      "          1824,    52,    17,   599,   345,    61,  1768,  4303,   599,     2,\n",
      "           834,     2,    61,    87,     7,  1824,    52,    17,   599,   567,\n",
      "           834,  1824,   201,     3,    18,   994,   102,   599,   382,   834,\n",
      "             2,  1935,     2,   834,     2,    61,  1768,  4303,   599,   157,\n",
      "            61,  1768,  3731,   599,     2,    61,    61,     1],\n",
      "        [  411,   834,   476,  3274,     3, 18650,   599,    31,   667,   834,\n",
      "           476,    31,    61,     3,     2,   834,     2,  3274,     3, 18650,\n",
      "           599,    31,     2,   834,     2,    31,    61,     3,     2,  3274,\n",
      "             3, 18650,   599,    31,     2,    31,    61,     3,    15,  3274,\n",
      "           262,  1824,   599,   509,     7,   599,   667,   834,   476,    61,\n",
      "          1768,   576,     7,   599,     2,   834,     2,   201,     3,     2,\n",
      "          1768,  4303,   599,   667,   834,   476,    61,    61,     1,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "batch = data_collator([tokenized_dataset[\"train\"][i] for i in range(1,4)])\n",
    "print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 88.64759993490114,\n",
       " 'counts': [61, 59, 56, 53],\n",
       " 'totals': [66, 65, 64, 63],\n",
       " 'precisions': [92.42424242424242, 90.76923076923077, 87.5, 84.12698412698413],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 66,\n",
       " 'ref_len': 61}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "predictions = [\"η = Symbol('η')\\nη_0 = Symbol('η_0')\\nQ_η = Symbol('Q_η')\\nR = Symbol('R')\\nT = Symbol('T')\\ne = Eq(η, η_0*exp(((Q_η*T)/(R*T)))\"]\n",
    "references = [[\"η = Symbol('η')\\nη_0 = Symbol('η_0')\\nQ_η = Symbol('Q_η')\\nR = Symbol('R')\\nT = Symbol('T')\\ne = Eq(η, η_0*exp(Q_η/(R*T)))\"]]\n",
    "\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    # SacreBLEU\n",
    "    BLEUresult = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Equation evaluation\n",
    "    \n",
    "\n",
    "    return {\"bleu\": BLEUresult[\"score\"]}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\kyanj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_login_key = os.environ.get(\"HF_LOGIN_KEY\")\n",
    "login(token=hf_login_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"t5-base-mathml-to-python\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args, \n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac12913ebd48442ebd509fa705191b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 17669.976, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.003, 'train_loss': 2.329975128173828, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=48, training_loss=2.329975128173828, metrics={'train_runtime': 17669.976, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.003, 'total_flos': 1311167215595520.0, 'train_loss': 2.329975128173828, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6c475023654f4f96b1ab6af28ed392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OverflowError",
     "evalue": "can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-base-mathml-to-python\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      8\u001b[0m     model, \n\u001b[0;32m      9\u001b[0m     args, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate(max_length\u001b[38;5;241m=\u001b[39mmax_length)\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:180\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys, metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix)\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[0;32m   3869\u001b[0m     eval_dataloader,\n\u001b[0;32m   3870\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3871\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[0;32m   3872\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[0;32m   3873\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3874\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[0;32m   3875\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[0;32m   3876\u001b[0m )\n\u001b[0;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer.py:4160\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4156\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   4157\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   4158\u001b[0m         )\n\u001b[0;32m   4159\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4160\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels))\n\u001b[0;32m   4161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4162\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_preds)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m      4\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m decoded_preds \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(preds, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Replace -100 in the labels as we can't decode them\u001b[39;00m\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, labels, tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3959\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3936\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3937\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3940\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3943\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3944\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3957\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3958\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   3960\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3961\u001b[0m             seq,\n\u001b[0;32m   3962\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3963\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3964\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3965\u001b[0m         )\n\u001b[0;32m   3966\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3967\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3960\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3936\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3937\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3940\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3943\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3944\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3957\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3958\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m-> 3960\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3961\u001b[0m             seq,\n\u001b[0;32m   3962\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3963\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3964\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3965\u001b[0m         )\n\u001b[0;32m   3966\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3967\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3999\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3996\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   3997\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 3999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   4000\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   4001\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   4002\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   4003\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4004\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:654\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    653\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[1;32m--> 654\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens)\n\u001b[0;32m    656\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    657\u001b[0m     clean_up_tokenization_spaces\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[0;32m    660\u001b[0m )\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[1;31mOverflowError\u001b[0m: can't convert negative int to unsigned"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Load trained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base-mathml-to-python\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args, \n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '         '}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model_checkpoint = \"t5-base-mathml-to-python\"\n",
    "translator = pipeline(\"text2text-generation\", model=model_checkpoint)\n",
    "result = translator(\"translate: MathML to Python: \\n<mml:mi>x</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mn>5</mml:mn>\")\n",
    "print(result)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<mml:mi>η</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:msub>\\n    <mml:mi>η</mml:mi>\\n    <mml:mi>0</mml:mi>\\n</mml:msub>\\n<mml:msup>\\n    <mml:mtext>exp</mml:mtext>\\n    <mml:mrow>\\n    <mml:mfrac>\\n        <mml:msub>\\n        <mml:mi>Q</mml:mi>\\n        <mml:mi>η</mml:mi>\\n        </mml:msub>\\n        <mml:mrow>\\n        <mml:mi>R</mml:mi>\\n        <mml:mi>T</mml:mi>\\n        </mml:mrow>\\n    </mml:mfrac>\\n    </mml:mrow>\\n</mml:msup>\\n</mml:mrow>'\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"<mml:mi>η</mml:mi>\n",
    "<mml:mo>=</mml:mo>\n",
    "<mml:mrow>\n",
    "<mml:msub>\n",
    "    <mml:mi>η</mml:mi>\n",
    "    <mml:mi>0</mml:mi>\n",
    "</mml:msub>\n",
    "<mml:msup>\n",
    "    <mml:mtext>exp</mml:mtext>\n",
    "    <mml:mrow>\n",
    "    <mml:mfrac>\n",
    "        <mml:msub>\n",
    "        <mml:mi>Q</mml:mi>\n",
    "        <mml:mi>η</mml:mi>\n",
    "        </mml:msub>\n",
    "        <mml:mrow>\n",
    "        <mml:mi>R</mml:mi>\n",
    "        <mml:mi>T</mml:mi>\n",
    "        </mml:mrow>\n",
    "    </mml:mfrac>\n",
    "    </mml:mrow>\n",
    "</mml:msup>\n",
    "</mml:mrow>\"\"\"\n",
    "\n",
    "print(repr(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁translate', '▁English', '▁to', '▁French', ':', '▁I', '▁love', '▁going', '▁to', '▁the', '▁park', '▁on', '▁the', '▁weekend']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Je suis enchanté d'aller au parc le week-end</s>\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"kj821/mathml-py-tokenizer-unigram-T5wrapped\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# text = \"\\n<mml:mi>h</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:msub>\\n<mml:mi>h</mml:mi>\\n<mml:mi>c</mml:mi>\\n</mml:msub>\\n<mml:mo>+</mml:mo>\\n<mml:msub>\\n<mml:mi>h</mml:mi>\\n<mml:mi>g</mml:mi>\\n</mml:msub>\\n</mml:mrow>\\n\"\n",
    "text = \"I love going to the park on the weekend\"\n",
    "prefix = \"translate English to French: \"\n",
    "input_ids = tokenizer.encode(prefix + text, return_tensors=\"pt\")\n",
    "print(tokenizer.tokenize(prefix + text))\n",
    "check = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "\n",
    "output_ids = model.generate(input_ids)\n",
    "output = tokenizer.decode(output_ids[0], skip_special_tokens=False, max_length=100)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
