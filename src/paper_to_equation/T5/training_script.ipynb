{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from src.paper_to_equation.Generation.Equation_BaseDataset import BaseDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5Tokenizer, T5TokenizerFast, T5ForConditionalGeneration\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import wandb\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 100%|██████████| 1000/1000 [00:10<00:00, 91.22it/s]\n",
      "Generating dataset: 100%|██████████| 200/200 [00:02<00:00, 97.56it/s] \n",
      "Generating dataset: 100%|██████████| 200/200 [00:02<00:00, 96.52it/s] \n"
     ]
    }
   ],
   "source": [
    "class T5Dataset(BaseDataset):\n",
    "    def __init__(self, num):\n",
    "        super().__init__(num)\n",
    "\n",
    "    def get_columns(self):\n",
    "        return [\"MathML\", \"Python\"]\n",
    "\n",
    "t5_data = T5Dataset(1000)\n",
    "t5_data.create(\"Data/t5_train_2.csv\")\n",
    "t5_data = T5Dataset(200)\n",
    "t5_data.create(\"Data/t5_validation_2.csv\")\n",
    "t5_data = T5Dataset(200)\n",
    "t5_data.create(\"Data/t5_test_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['MathML', 'Python'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['MathML', 'Python'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['MathML', 'Python'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": \"Data/t5_train_2.csv\",\n",
    "    \"validation\": \"Data/t5_validation_2.csv\",\n",
    "    \"test\": \"Data/t5_test_2.csv\"\n",
    "    }\n",
    "mml_py_dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "def remove_carriage_return(examples):\n",
    "    return {\n",
    "        \"MathML\": examples[\"MathML\"].replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\"),\n",
    "        \"Python\": examples[\"Python\"].replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\")\n",
    "    }\n",
    "\n",
    "mml_py_dataset.map(remove_carriage_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"kj821/mathml-py-tokenizer-sentencepiece-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [46, 5, 40, 39, 24, 167, 27, 40, 39, 24, 130, 27, 40, 39, 9, 40, 39, 16, 47, 11, 40, 39, 25, 40, 39, 19, 40, 39, 16, 161, 11, 40, 39, 25, 40, 39, 24, 62, 27, 40, 39, 16, 47, 11, 40, 39, 6, 163, 14, 40, 39, 18, 40, 39, 5, 40, 39, 24, 134, 27, 40, 39, 25, 40, 39, 24, 170, 27, 40, 39, 24, 147, 27, 40, 39, 24, 141, 27, 40, 39, 18, 40, 39, 9, 40, 39, 21, 40, 39, 17, 40, 39, 25, 40, 39, 7, 40, 39, 7, 40, 39, 26, 30, 10, 40, 39, 24, 137, 27, 40, 39, 12, 40, 39, 5, 40, 39, 24, 78, 27, 40, 39, 25, 40, 39, 24, 64, 27, 40, 39, 24, 100, 27, 40, 39, 18, 40, 39, 9, 40, 39, 12, 40, 39, 16, 54, 11, 40, 39, 25, 40, 39, 24, 33, 27, 40, 39, 17, 40, 39, 5, 40, 39, 24, 126, 27, 40, 39, 25, 40, 39, 24, 129, 27, 40, 39, 24, 82, 27, 40, 39, 18, 40, 39, 9, 40, 39, 4, 40, 39, 18, 40, 39, 18, 40, 39, 4, 40, 39, 18, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [46, 167, 49, 130, 46, 47, 46, 28, 52, 167, 49, 130, 50, 48, 40, 39, 62, 46, 47, 46, 28, 52, 62, 50, 48, 40, 39, 134, 49, 170, 147, 141, 46, 47, 46, 28, 52, 134, 49, 170, 147, 141, 50, 48, 40, 39, 78, 49, 64, 100, 46, 47, 46, 28, 52, 78, 49, 64, 100, 50, 48, 40, 39, 137, 46, 47, 46, 28, 52, 137, 50, 48, 40, 39, 126, 49, 129, 82, 46, 47, 46, 28, 52, 126, 49, 129, 82, 50, 48, 40, 39, 55, 46, 47, 46, 29, 51, 167, 49, 130, 53, 46, 36, 51, 30, 51, 137, 173, 78, 49, 64, 100, 46, 54, 46, 33, 51, 126, 49, 129, 82, 48, 53, 46, 51, 62, 53, 46, 163, 53, 46, 134, 49, 170, 147, 141, 165, 2]}\n",
      "<mml:msub>\n",
      "<mml:mi>N</mml:mi>\n",
      "<mml:mi>P</mml:mi>\n",
      "</mml:msub>\n",
      "<mml:mo>=</mml:mo>\n",
      "<mml:mrow>\n",
      "<mml:munderover>\n",
      "<mml:mo>∑</mml:mo>\n",
      "<mml:mrow>\n",
      "<mml:mi>t</mml:mi>\n",
      "<mml:mo>=</mml:mo>\n",
      "<mml:mn>7</mml:mn>\n",
      "</mml:mrow>\n",
      "<mml:msub>\n",
      "<mml:mi>Ο</mml:mi>\n",
      "<mml:mrow>\n",
      "<mml:mi>O</mml:mi>\n",
      "<mml:mi>ι</mml:mi>\n",
      "<mml:mi>χ</mml:mi>\n",
      "</mml:mrow>\n",
      "</mml:msub>\n",
      "</mml:munderover>\n",
      "<mml:mfenced>\n",
      "<mml:mrow>\n",
      "<mml:msup>\n",
      "<mml:msup>\n",
      "<mml:mtext>exp</mml:mtext>\n",
      "<mml:mi>φ</mml:mi>\n",
      "</mml:msup>\n",
      "<mml:msub>\n",
      "<mml:mi>Μ</mml:mi>\n",
      "<mml:mrow>\n",
      "<mml:mi>ω</mml:mi>\n",
      "<mml:mi>L</mml:mi>\n",
      "</mml:mrow>\n",
      "</mml:msub>\n",
      "</mml:msup>\n",
      "<mml:mo>+</mml:mo>\n",
      "<mml:mrow>\n",
      "<mml:mi>tan</mml:mi>\n",
      "<mml:mfenced>\n",
      "<mml:msub>\n",
      "<mml:mi>η</mml:mi>\n",
      "<mml:mrow>\n",
      "<mml:mi>y</mml:mi>\n",
      "<mml:mi>λ</mml:mi>\n",
      "</mml:mrow>\n",
      "</mml:msub>\n",
      "</mml:mfenced>\n",
      "</mml:mrow>\n",
      "</mml:mrow>\n",
      "</mml:mfenced>\n",
      "</mml:mrow>\n"
     ]
    }
   ],
   "source": [
    "mml_sentence = mml_py_dataset[\"train\"][\"MathML\"][0]\n",
    "py_sentence = mml_py_dataset[\"train\"][\"Python\"][0]\n",
    "\n",
    "inputs = tokenizer(mml_sentence, text_target=py_sentence)\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs[\"input_ids\"], skip_special_tokens=True))\n",
    "# print(tokenizer.encode(mml_sentence))\n",
    "# print(tokenizer.decode(inputs[\"labels\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 512\n",
    "def preprocess_function(examples):\n",
    "    prefix = \"translate MathML to Python: \"\n",
    "    inputs = [prefix + mml for mml in examples[\"MathML\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"Python\"], max_length=max_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = mml_py_dataset.map(preprocess_function, batched=True, remove_columns=[\"MathML\", \"Python\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 167, 49, 130, 46, 47, 46, 28, 52, 167, 49, 130, 50, 48, 40, 39, 62, 46, 47, 46, 28, 52, 62, 50, 48, 40, 39, 134, 49, 170, 147, 141, 46, 47, 46, 28, 52, 134, 49, 170, 147, 141, 50, 48, 40, 39, 78, 49, 64, 100, 46, 47, 46, 28, 52, 78, 49, 64, 100, 50, 48, 40, 39, 137, 46, 47, 46, 28, 52, 137, 50, 48, 40, 39, 126, 49, 129, 82, 46, 47, 46, 28, 52, 126, 49, 129, 82, 50, 48, 40, 39, 55, 46, 47, 46, 29, 51, 167, 49, 130, 53, 46, 36, 51, 30, 51, 137, 173, 78, 49, 64, 100, 46, 54, 46, 33, 51, 126, 49, 129, 82, 48, 53, 46, 51, 62, 53, 46, 163, 53, 46, 134, 49, 170, 147, 141, 165, 2]\n",
      "N_P = Symbol('N_P')\n",
      "t = Symbol('t')\n",
      "Ο_Oιχ = Symbol('Ο_Oιχ')\n",
      "Μ_ωL = Symbol('Μ_ωL')\n",
      "φ = Symbol('φ')\n",
      "η_yλ = Symbol('η_yλ')\n",
      "e = Eq(N_P, Sum(exp(φ)**Μ_ωL + tan(η_yλ), (t, 7, Ο_Oιχ)))\n"
     ]
    }
   ],
   "source": [
    "ids = tokenized_dataset[\"train\"][0][\"labels\"]\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  46,   67,   46,   47,   46,   28,   52,   67,   50,   48,   40,   39,\n",
      "           92,   46,   47,   46,   28,   52,   92,   50,   48,   40,   39,  130,\n",
      "           49,  148,  139,  102,   46,   47,   46,   28,   52,  130,   49,  148,\n",
      "          139,  102,   50,   48,   40,   39,   55,   46,   47,   46,   29,   51,\n",
      "           67,   53,   46,   37,  175,   35,   51,   92,   48,   46,   54,   46,\n",
      "           31,   51,  130,   49,  148,  139,  102,   48,   53,   46,  130,   49,\n",
      "          148,  139,  102,   58,    2, -100, -100, -100, -100],\n",
      "        [  46,   94,   46,   47,   46,   28,   52,   94,   50,   48,   40,   39,\n",
      "           75,   46,   47,   46,   28,   52,   75,   50,   48,   40,   39,  153,\n",
      "          186,   57,   66,   46,   47,   46,   28,   52,  153,  186,   57,   66,\n",
      "           50,   48,   40,   39,   55,   46,   47,   46,   29,   51,   94,   53,\n",
      "           46,   37,   51,   35,   51,   75,  172,  153,  186,   57,   66,   53,\n",
      "           46,  153,  186,   57,   66,   58,    2, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [  46,  150,   49,   66,  134,   46,   47,   46,   28,   52,  150,   49,\n",
      "           66,  134,   50,   48,   40,   39,  102,   46,   47,   46,   28,   52,\n",
      "          102,   50,   48,   40,   39,   98,   46,   47,   46,   28,   52,   98,\n",
      "           50,   48,   40,   39,  115,   46,   47,   46,   28,   52,  115,   50,\n",
      "           48,   40,   39,   55,   46,   47,   46,   29,   51,  150,   49,   66,\n",
      "          134,   53,   46,   36,  239,   31,   51,  115,   48,   53,   46,   51,\n",
      "          102,   53,   46,  158,   53,   46,   98,  165,    2]])\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "batch = data_collator([tokenized_dataset[\"train\"][i] for i in range(1,4)])\n",
    "print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 88.64759993490114,\n",
       " 'counts': [61, 59, 56, 53],\n",
       " 'totals': [66, 65, 64, 63],\n",
       " 'precisions': [92.42424242424242, 90.76923076923077, 87.5, 84.12698412698413],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 66,\n",
       " 'ref_len': 61}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "predictions = [\"η = Symbol('η')\\nη_0 = Symbol('η_0')\\nQ_η = Symbol('Q_η')\\nR = Symbol('R')\\nT = Symbol('T')\\ne = Eq(η, η_0*exp(((Q_η*T)/(R*T)))\"]\n",
    "references = [[\"η = Symbol('η')\\nη_0 = Symbol('η_0')\\nQ_η = Symbol('Q_η')\\nR = Symbol('R')\\nT = Symbol('T')\\ne = Eq(η, η_0*exp(Q_η/(R*T)))\"]]\n",
    "\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    # SacreBLEU\n",
    "    BLEUresult = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Equation evaluation\n",
    "    \n",
    "\n",
    "    return {\"bleu\": BLEUresult[\"score\"]}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_login_key = os.environ.get(\"HF_LOGIN_KEY\")\n",
    "login(token=hf_login_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\kyanj\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkj821\u001b[0m (\u001b[33mkj821-imperial-college-london\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb_api_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_api_key)\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"mathml-python\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"checkpoint\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = f\"Model_Files/t5-small-mathml-python-v1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=\"t5-small-mathml-python-v1\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args, \n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac12913ebd48442ebd509fa705191b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 17669.976, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.003, 'train_loss': 2.329975128173828, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=48, training_loss=2.329975128173828, metrics={'train_runtime': 17669.976, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.003, 'total_flos': 1311167215595520.0, 'train_loss': 2.329975128173828, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6c475023654f4f96b1ab6af28ed392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OverflowError",
     "evalue": "can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-base-mathml-to-python\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      8\u001b[0m     model, \n\u001b[0;32m      9\u001b[0m     args, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate(max_length\u001b[38;5;241m=\u001b[39mmax_length)\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:180\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys, metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix)\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[0;32m   3869\u001b[0m     eval_dataloader,\n\u001b[0;32m   3870\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3871\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[0;32m   3872\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[0;32m   3873\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3874\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[0;32m   3875\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[0;32m   3876\u001b[0m )\n\u001b[0;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\trainer.py:4160\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4156\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   4157\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   4158\u001b[0m         )\n\u001b[0;32m   4159\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4160\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels))\n\u001b[0;32m   4161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4162\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_preds)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m      4\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m decoded_preds \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(preds, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Replace -100 in the labels as we can't decode them\u001b[39;00m\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, labels, tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3959\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3936\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3937\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3940\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3943\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3944\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3957\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3958\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   3960\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3961\u001b[0m             seq,\n\u001b[0;32m   3962\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3963\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3964\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3965\u001b[0m         )\n\u001b[0;32m   3966\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3967\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3960\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3936\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3937\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3940\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3943\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3944\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3957\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3958\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m-> 3960\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3961\u001b[0m             seq,\n\u001b[0;32m   3962\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3963\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3964\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3965\u001b[0m         )\n\u001b[0;32m   3966\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3967\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3999\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3996\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   3997\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 3999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   4000\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   4001\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   4002\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   4003\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4004\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:654\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    653\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[1;32m--> 654\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens)\n\u001b[0;32m    656\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    657\u001b[0m     clean_up_tokenization_spaces\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[0;32m    660\u001b[0m )\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[1;31mOverflowError\u001b[0m: can't convert negative int to unsigned"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Load trained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base-mathml-to-python\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args, \n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '         '}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model_checkpoint = \"t5-base-mathml-to-python\"\n",
    "translator = pipeline(\"text2text-generation\", model=model_checkpoint)\n",
    "result = translator(\"translate: MathML to Python: \\n<mml:mi>x</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mn>5</mml:mn>\")\n",
    "print(result)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<mml:mi>η</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:msub>\\n    <mml:mi>η</mml:mi>\\n    <mml:mi>0</mml:mi>\\n</mml:msub>\\n<mml:msup>\\n    <mml:mtext>exp</mml:mtext>\\n    <mml:mrow>\\n    <mml:mfrac>\\n        <mml:msub>\\n        <mml:mi>Q</mml:mi>\\n        <mml:mi>η</mml:mi>\\n        </mml:msub>\\n        <mml:mrow>\\n        <mml:mi>R</mml:mi>\\n        <mml:mi>T</mml:mi>\\n        </mml:mrow>\\n    </mml:mfrac>\\n    </mml:mrow>\\n</mml:msup>\\n</mml:mrow>'\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"<mml:mi>η</mml:mi>\n",
    "<mml:mo>=</mml:mo>\n",
    "<mml:mrow>\n",
    "<mml:msub>\n",
    "    <mml:mi>η</mml:mi>\n",
    "    <mml:mi>0</mml:mi>\n",
    "</mml:msub>\n",
    "<mml:msup>\n",
    "    <mml:mtext>exp</mml:mtext>\n",
    "    <mml:mrow>\n",
    "    <mml:mfrac>\n",
    "        <mml:msub>\n",
    "        <mml:mi>Q</mml:mi>\n",
    "        <mml:mi>η</mml:mi>\n",
    "        </mml:msub>\n",
    "        <mml:mrow>\n",
    "        <mml:mi>R</mml:mi>\n",
    "        <mml:mi>T</mml:mi>\n",
    "        </mml:mrow>\n",
    "    </mml:mfrac>\n",
    "    </mml:mrow>\n",
    "</mml:msup>\n",
    "</mml:mrow>\"\"\"\n",
    "\n",
    "print(repr(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'ra', 'n', 's', 'l', 'a', 'te', ':', ' ', 'M', 'a', 't', 'h', 'M', 'L', ' ', 't', 'o', ' ', 'P', 'y', 't', 'h', 'o', 'n', ':', ' ', '\\n', '<mml:mi>', 'h', '</mml:mi>', '\\n', '<mml:mo>', '=', '</mml:mo>', '\\n', '<mml:mrow>', '\\n', '<mml:msub>', '\\n', '<mml:mi>', 'h', '</mml:mi>', '\\n', '<mml:mi>', 'c', '</mml:mi>', '\\n', '</mml:msub>', '\\n', '<mml:mo>', '+', '</mml:mo>', '\\n', '<mml:msub>', '\\n', '<mml:mi>', 'h', '</mml:mi>', '\\n', '<mml:mi>', 'g', '</mml:mi>', '\\n', '</mml:msub>', '\\n', '</mml:mrow>', '\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyanj\\anaconda3\\envs\\fyp_env\\Lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>: : : \n",
      "<mml:mi>: \n",
      "<mml:mi>: \n",
      "<mml:mi>: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kj821/mathml-py-tokenizer-unigram-T5wrapped\")\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"kj821/t5-base-mathml-to-python\")\n",
    "\n",
    "text = \"\\n<mml:mi>h</mml:mi>\\n<mml:mo>=</mml:mo>\\n<mml:mrow>\\n<mml:msub>\\n<mml:mi>h</mml:mi>\\n<mml:mi>c</mml:mi>\\n</mml:msub>\\n<mml:mo>+</mml:mo>\\n<mml:msub>\\n<mml:mi>h</mml:mi>\\n<mml:mi>g</mml:mi>\\n</mml:msub>\\n</mml:mrow>\\n\"\n",
    "# text = \"I love going to the park on the weekend\"\n",
    "prefix = \"translate: MathML to Python: \"\n",
    "input_ids = tokenizer.encode(prefix + text, return_tensors=\"pt\")\n",
    "print(tokenizer.tokenize(prefix + text))\n",
    "check = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "\n",
    "output_ids = model.generate(input_ids)\n",
    "output = tokenizer.decode(output_ids[0], skip_special_tokens=False, max_new_tokens=100)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
