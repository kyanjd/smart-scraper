{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "from transformers import T5Tokenizer\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer, Regex\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import Metaspace, Split, WhitespaceSplit\n",
    "from huggingface_hub import login\n",
    "\n",
    "import re\n",
    "import src.paper_to_equation.Generation.Equation_BaseDataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(src.paper_to_equation.Generation.Equation_BaseDataset)\n",
    "from src.paper_to_equation.Generation.Equation_BaseDataset import BaseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming the default tokenizer is unsuitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'h', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', 'mm', 'l', ':', 'm', 'o', '>', '=', '<', '/', 'mm', 'l', ':', 'm', 'o', '>', '▁', '<', 'mm', 'l', ':', 'm', 'row', '>', '▁', '<', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'h', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'c', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', '/', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', 'mm', 'l', ':', 'm', 'o', '>', '+', '<', '/', 'mm', 'l', ':', 'm', 'o', '>', '▁', '<', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'h', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', 'mm', 'l', ':', 'm', 'i', '>', 'g', '<', '/', 'mm', 'l', ':', 'm', 'i', '>', '▁', '<', '/', 'mm', 'l', ':', 'm', 'sub', '>', '▁', '<', '/', 'mm', 'l', ':', 'm', 'row', '>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "mml = \"\"\"\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mo>=</mml:mo>\n",
    "<mml:mrow>\n",
    "<mml:msub>\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mi>c</mml:mi>\n",
    "</mml:msub>\n",
    "<mml:mo>+</mml:mo>\n",
    "<mml:msub>\n",
    "<mml:mi>h</mml:mi>\n",
    "<mml:mi>g</mml:mi>\n",
    "</mml:msub>\n",
    "</mml:mrow>\n",
    "  \"\"\"\n",
    "\n",
    "py = \"\"\"\n",
    "h = Symbol('h')\n",
    "h_g = Symbol('h_g')\n",
    "h_c = Symbol('h_c')\n",
    "e = Eq(h, h_g + h_c)\"\"\"\n",
    "\n",
    "tokens = tokenizer.tokenize(mml)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerDataset(BaseDataset):\n",
    "    def __init__(self, num):\n",
    "        super().__init__(num)\n",
    "\n",
    "    def get_columns(self):\n",
    "        return [\"mathml\", \"python\"]\n",
    "\n",
    "    def map_atomic_tokens(self, dataset):\n",
    "\n",
    "        tag_map = {\"<mml:mo>\": \"<MO>\", \"</mml:mo>\": \"</MO>\",\n",
    "                   \"<mml:mi>\": \"<MI>\", \"</mml:mi>\": \"</MI>\",\n",
    "                   \"<mml:msub>\": \"<MSUB>\", \"</mml:msub>\": \"</MSUB>\",\n",
    "                   \"<mml:msup>\": \"<MSUP>\", \"</mml:msup>\": \"</MSUP>\",\n",
    "                   \"<mml:mrow>\": \"<MROW>\", \"</mml:mrow>\": \"</MROW>\", \n",
    "                   \"<mml:mfrac>\": \"<MFRAC>\", \"</mml:mfrac>\": \"</MFRAC>\"}\n",
    "        \n",
    "        for entry in dataset:\n",
    "            mathml = entry[\"mathml\"]\n",
    "            for tag, token in tag_map.items():\n",
    "                mathml = mathml.replace(tag, token)\n",
    "            entry[\"mathml\"] = mathml\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def get_tag_list(self):\n",
    "        return [\"<mml:mo>\", \"</mml:mo>\",\n",
    "                \"<mml:mi>\", \"</mml:mi>\",\n",
    "                \"<mml:msub>\", \"</mml:msub>\",\n",
    "                \"<mml:msup>\", \"</mml:msup>\",\n",
    "                \"<mml:mrow>\", \"</mml:mrow>\",\n",
    "                \"<mml:mfrac>\", \"</mml:mfrac>\",\n",
    "                \"<mml:mtext>\", \"</mml:mtext>\"]\n",
    "    \n",
    "    def extract_tags(self, data):\n",
    "        tags = set()\n",
    "        for entry in data:\n",
    "            mathml = entry[\"mathml\"]\n",
    "            for tag in mathml:\n",
    "                if tag not in tags:\n",
    "                    tags.append(tag)\n",
    "        return tags\n",
    "        \n",
    "    def data_iterator(self, batch_size):\n",
    "        columns = self.get_columns()\n",
    "        for i in range(0, len(self.dataset), batch_size):\n",
    "            yield [f\"{data[columns[0]]} {data[columns[1]]} \"for data in self.dataset[i:i+batch_size]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 100%|██████████| 1000/1000 [00:11<00:00, 89.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "1 5\n",
      "2 5\n",
      "3 5\n",
      "4 5\n",
      "5 5\n",
      "6 5\n",
      "7 5\n",
      "8 5\n",
      "9 5\n",
      "10 5\n",
      "11 5\n",
      "12 5\n",
      "13 5\n",
      "14 5\n",
      "15 5\n",
      "16 5\n",
      "17 5\n",
      "18 5\n",
      "19 5\n",
      "20 5\n",
      "21 5\n",
      "22 5\n",
      "23 5\n",
      "24 5\n",
      "25 5\n",
      "26 5\n",
      "27 5\n",
      "28 5\n",
      "29 5\n",
      "30 5\n",
      "31 5\n",
      "32 5\n",
      "33 5\n",
      "34 5\n",
      "35 5\n",
      "36 5\n",
      "37 5\n",
      "38 5\n",
      "39 5\n",
      "40 5\n",
      "41 5\n",
      "42 5\n",
      "43 5\n",
      "44 5\n",
      "45 5\n",
      "46 5\n",
      "47 5\n",
      "48 5\n",
      "49 5\n",
      "50 5\n",
      "51 5\n",
      "52 5\n",
      "53 5\n",
      "54 5\n",
      "55 5\n",
      "56 5\n",
      "57 5\n",
      "58 5\n",
      "59 5\n",
      "60 5\n",
      "61 5\n",
      "62 5\n",
      "63 5\n",
      "64 5\n",
      "65 5\n",
      "66 5\n",
      "67 5\n",
      "68 5\n",
      "69 5\n",
      "70 5\n",
      "71 5\n",
      "72 5\n",
      "73 5\n",
      "74 5\n",
      "75 5\n",
      "76 5\n",
      "77 5\n",
      "78 5\n",
      "79 5\n",
      "80 5\n",
      "81 5\n",
      "82 5\n",
      "83 5\n",
      "84 5\n",
      "85 5\n",
      "86 5\n",
      "87 5\n",
      "88 5\n",
      "89 5\n",
      "90 5\n",
      "91 5\n",
      "92 5\n",
      "93 5\n",
      "94 5\n",
      "95 5\n",
      "96 5\n",
      "97 5\n",
      "98 5\n",
      "99 5\n",
      "100 5\n",
      "101 5\n",
      "102 5\n",
      "103 5\n",
      "104 5\n",
      "105 5\n",
      "106 5\n",
      "107 5\n",
      "108 5\n",
      "109 5\n",
      "110 5\n",
      "111 5\n",
      "112 5\n",
      "113 5\n",
      "114 5\n",
      "115 5\n",
      "116 5\n",
      "117 5\n",
      "118 5\n",
      "119 5\n",
      "120 5\n",
      "121 5\n",
      "122 5\n",
      "123 5\n",
      "124 5\n",
      "125 5\n",
      "126 5\n",
      "127 5\n",
      "128 5\n",
      "129 5\n",
      "130 5\n",
      "131 5\n",
      "132 5\n",
      "133 5\n",
      "134 5\n",
      "135 5\n",
      "136 5\n",
      "137 5\n",
      "138 5\n",
      "139 5\n",
      "140 5\n",
      "141 5\n",
      "142 5\n",
      "143 5\n",
      "144 5\n",
      "145 5\n",
      "146 5\n",
      "147 5\n",
      "148 5\n",
      "149 5\n",
      "150 5\n",
      "151 5\n",
      "152 5\n",
      "153 5\n",
      "154 5\n",
      "155 5\n",
      "156 5\n",
      "157 5\n",
      "158 5\n",
      "159 5\n",
      "160 5\n",
      "161 5\n",
      "162 5\n",
      "163 5\n",
      "164 5\n",
      "165 5\n",
      "166 5\n",
      "167 5\n",
      "168 5\n",
      "169 5\n",
      "170 5\n",
      "171 5\n",
      "172 5\n",
      "173 5\n",
      "174 5\n",
      "175 5\n",
      "176 5\n",
      "177 5\n",
      "178 5\n",
      "179 5\n",
      "180 5\n",
      "181 5\n",
      "182 5\n",
      "183 5\n",
      "184 5\n",
      "185 5\n",
      "186 5\n",
      "187 5\n",
      "188 5\n",
      "189 5\n",
      "190 5\n",
      "191 5\n",
      "192 5\n",
      "193 5\n",
      "194 5\n",
      "195 5\n",
      "196 5\n",
      "197 5\n",
      "198 5\n",
      "199 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "td = TokenizerDataset(1000)\n",
    "td.clear_dataset()\n",
    "td.create_dataset()\n",
    "data_iterator = td.data_iterator(5)\n",
    "\n",
    "for i, data in enumerate(data_iterator):\n",
    "    print(i, len(data))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁\\nh', (0, 2)),\n",
       " ('▁=', (2, 4)),\n",
       " (\"▁Symbol('h')\\nh_g\", (4, 20)),\n",
       " ('▁=', (20, 22)),\n",
       " (\"▁Symbol('h_g')\\nh_c\", (22, 40)),\n",
       " ('▁=', (40, 42)),\n",
       " (\"▁Symbol('h_c')\\ne\", (42, 58)),\n",
       " ('▁=', (58, 60)),\n",
       " ('▁Eq(h,', (60, 66)),\n",
       " ('▁h_g', (66, 70)),\n",
       " ('▁+', (70, 72)),\n",
       " ('▁h_c)', (72, 77))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"')] # don't lowercase\n",
    ")\n",
    "tokenizer.normalizer.normalize_str(py)\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No constructor defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m\n\u001b[0;32m     45\u001b[0m                 splits\u001b[38;5;241m.\u001b[39mappend((text[last_end:], last_end \u001b[38;5;241m+\u001b[39m offset))\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m splits\n\u001b[1;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m MathMLPyTokenizer()\n\u001b[0;32m     51\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(mml)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: No constructor defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())                  \n",
    "\n",
    "class MathMLPyTokenizer(pre_tokenizers.PreTokenizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pre_tokenize(self, data):\n",
    "        \n",
    "        patterns = [\n",
    "            # HTML/XML tags\n",
    "            r'<[^>]+>',\n",
    "            # Python string literals\n",
    "            r'\"\"\"[^\"]*\"\"\"', r\"'''[^']*'''\", r'\"[^\"]*\"', r\"'[^']*'\",\n",
    "            # Python keywords and operators\n",
    "            r'\\bdef\\b', r'\\bclass\\b', r'\\bfor\\b', r'\\bwhile\\b', r'\\bif\\b', r'\\belif\\b', r'\\belse\\b',\n",
    "            r'\\breturn\\b', r'\\bimport\\b', r'\\bfrom\\b', r'\\bas\\b', r'\\bwith\\b', r'\\btry\\b', r'\\bexcept\\b',\n",
    "            # Common Python syntax elements\n",
    "            r'==', r'!=', r'<=', r'>=', r'\\+=', r'-=', r'\\*=', r'/=', \n",
    "            r'=>', r'->',  # Function type hints and lambdas\n",
    "            r'\\bSymbol\\b', r'\\bEq\\b', r'\\bexp\\b', r'\\bsin\\b', r'\\bcos\\b', r'\\btan\\b', r'\\bdiff\\b',\n",
    "            \n",
    "            # Indentation (important for Python)\n",
    "            r'^\\s+'\n",
    "        ]\n",
    "\n",
    "        combined_pattern = '|'.join(f'({p})' for p in patterns)\n",
    "        regex = re.compile(combined_pattern, re.MULTILINE)\n",
    "        \n",
    "        splits = []\n",
    "        for text, offset in data:\n",
    "            last_end = 0\n",
    "            for match in regex.finditer(text):\n",
    "                start, end = match.span()\n",
    "                \n",
    "                if start > last_end:\n",
    "                    # Add text before the special token\n",
    "                    splits.append((text[last_end:start], last_end + offset))\n",
    "                \n",
    "                # Add the special token as a whole\n",
    "                splits.append((text[start:end], start + offset))\n",
    "                last_end = end\n",
    "            \n",
    "            if last_end < len(text):\n",
    "                # Add remaining text\n",
    "                splits.append((text[last_end:], last_end + offset))\n",
    "                \n",
    "        return splits\n",
    "\n",
    "x = MathMLPyTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(mml)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 100%|██████████| 10000/10000 [01:39<00:00, 100.33it/s]\n",
      "Generating dataset:  92%|█████████▏| 9223/10000 [01:28<00:07, 104.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1:  99.67798781394958\n",
      "Time 2:  90.23344993591309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start1 = time.time()\n",
    "    td1 = TokenizerDataset(10000)\n",
    "    td1.create_dataset()\n",
    "    end1 = time.time()\n",
    "\n",
    "    start2 = time.time()\n",
    "    td2 = TokenizerDataset(10000)\n",
    "    td2.create_dataset_mthread()\n",
    "    end2 = time.time()\n",
    "\n",
    "    print(\"Time 1: \", end1 - start1)\n",
    "    print(\"Time 2: \", end2 - start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "td = TokenizerDataset(10000)\n",
    "td.create_dataset()\n",
    "print(len(td.dataset))\n",
    "data_iterator = td.data_iterator(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base tokenizer (BPE, WordPiece, or Unigram)\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Define patterns we want to protect during tokenization\n",
    "text_patterns = [\n",
    "            # HTML/XML tags\n",
    "            r'<[^>]+>',\n",
    "            # Python string literals\n",
    "            r'\"\"\"[^\"]*\"\"\"', r\"'''[^']*'''\", r'\"[^\"]*\"', r\"'[^']*'\",\n",
    "            # Python keywords and operators\n",
    "            r'\\bdef\\b', r'\\bclass\\b', r'\\bfor\\b', r'\\bwhile\\b', r'\\bif\\b', r'\\belif\\b', r'\\belse\\b',\n",
    "            r'\\breturn\\b', r'\\bimport\\b', r'\\bfrom\\b', r'\\bas\\b', r'\\bwith\\b', r'\\btry\\b', r'\\bexcept\\b',\n",
    "            # Common Python syntax elements\n",
    "            r'==', r'!=', r'<=', r'>=', r'\\+=', r'-=', r'\\*=', r'/=', \n",
    "            r'=>', r'->',  # Function type hints and lambdas\n",
    "            r'\\bSymbol\\b', r'\\bEq\\b', r'\\bexp\\b', r'\\bsin\\b', r'\\bcos\\b', r'\\btan\\b', r'\\bdiff\\b', # MathML\n",
    "            # Indentation (important for Python)\n",
    "            r'^\\s+',\n",
    "            r'\\s+' # Whitespace\n",
    "        ]\n",
    "\n",
    "# Set up the pre-tokenizer using Split with pattern\n",
    "pattern = '|'.join(text_patterns)\n",
    "split_pre_tokenizer = Split(pattern=Regex(pattern), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = split_pre_tokenizer\n",
    "\n",
    "# Set up other components \n",
    "tokenizer.normalizer = Sequence([NFKC()])\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "tag_list = td.get_tag_list()\n",
    "special_tokens = [\"[PAD]\", \"[BOS]\", \"[EOS]\", \"[UNK]\"] + tag_list\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(iterator=data_iterator, trainer=trainer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\kyanj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_login_key = os.environ.get(\"HF_LOGIN_KEY\")\n",
    "login(token=hf_login_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"Tokenizer_Files/mathml-py-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '<mml:mi>', 'h', '</mml:mi>', '\\n', '<mml:mo>', '=', '</mml:mo>', '\\n', '<mml:mrow>', '\\n', '<mml:msub>', '\\n', '<mml:mi>', 'h', '</mml:mi>', '\\n', '<mml:mi>', 'c', '</mml:mi>', '\\n', '</mml:msub>', '\\n', '<mml:mo>', '+', '</mml:mo>', '\\n', '<mml:msub>', '\\n', '<mml:mi>', 'h', '</mml:mi>', '\\n', '<mml:mi>', 'g', '</mml:mi>', '\\n', '</mml:msub>', '\\n', '</mml:mrow>', '\\n', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "output_mathml = tokenizer.encode(td.dataset[0][\"mathml\"])\n",
    "output_py = tokenizer.encode(td.dataset[0][\"python\"])\n",
    "# print(output_py.tokens)\n",
    "# print(output_py.tokens)\n",
    "# print(output_py.ids)\n",
    "# print(tokenizer.decode(output_py.ids))\n",
    "output_test = tokenizer.encode(mml)\n",
    "print(output_test.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['def my_function(x, y)', 'class MyClass', 'def method(self)', 'import numpy', '<math>', '<msup>', '<mi>', '</mi>', '<mn>', '</mn>', '</msup>', '</math>']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"\n",
    "def my_function(x, y): return x + y\n",
    "class MyClass:\n",
    "    def method(self): pass\n",
    "import numpy as np\n",
    "<math><msup><mi>x</mi><mn>2</mn></msup></math>\n",
    "\"\"\"\n",
    "\n",
    "check = tokenizer.pre_tokenizer.pre_tokenize_str(mml)\n",
    "print(check)\n",
    "output = tokenizer.encode(test_text)\n",
    "print(output.tokens)\n",
    "print(output.ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
